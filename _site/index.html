<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="icon" type="image/icon" href="favicon.ico">


<title>Home</title>


<script src="https://storage.googleapis.com/montco-stats/javascript/vis-4.17.0/dist/vis.js" type="text/javascript"></script>
<link href="https://storage.googleapis.com/montco-stats/javascript/vis-4.17.0/dist/vis-network.min.css" rel="stylesheet" type="text/css" />

<link rel="stylesheet" href="http://localhost:4000/css/syntax.css">
<link rel="stylesheet" href="http://localhost:4000/css/main.css">
</head>
<body>


<header>
<div id="nav_container">
<nav>

  <ul>
  <h1><a href="">Research Notes</a></h1>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/about/">About</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/tutorials/">Tutorials</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/paper_summary/">Paper Summaries</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/distill/">Distills</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/tags/">Archives</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/almanac/personal_log/">Almanac</a>
      </li>
    
  </ul>

</nav>
</div>
</header>

<section id="main">
	  
      <article>
      <div class="heading"><a href="/post/meta-learning-deep-energy-based-models/">Meta-Learning Deep Energy-Based Models</a></div>
      <p class="meta"><a class="permalink" href="/post/meta-learning-deep-energy-based-models/">&#9679;</a> 22 Oct 2019</p>
      <div>
        <p><a href="https://arxiv.org/abs/1910.02720">Paper link</a></p>

<p>They study the problem of learning associative memory – a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.</p>

<p>Attractor networks (Hopfield networks are a particular type) provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor.</p>

<p>In such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield networks.</p>

<p><img src="\photos\sergey_bartunov.png" alt="" style="width: 600px; height: auto;" /></p>

<p>Basically they have interpretation in terms of auto-encoder and minimize the reconstruction errors on distorted patterns to learn appropraite writing and retrieval rules.</p>

<p>\[ \underset{\theta, r, \tau}{\mathop{min}}~\mathbb{E}_{X\sim p(X)} \left[\mathcal{L}\left(X, \mathop{read}\left(\tilde{X};\mathop{write}(X)\right)\right)\right].\]</p>

<p>Here writing loss is defined as:</p>

<script type="math/tex; mode=display">\mathcal{W}\left(x, \theta\right)= \mathcal{E}\left(x, \theta\right) + \alpha ||\nabla_{x}\mathcal{E}\left(x, \theta\right) ||_{2}^{2}+ \beta ||\theta - \bar{\theta}||_{2}^{2}.</script>

<p>The Early-stopping gradient descent is implement on writing loss to implement the wrting rule:</p>

<p>\[ \mathop{write}(X)=\theta^{(T)},~~\theta^{(T+1)}=\theta^{(T)}- \eta^{(T)} \frac{1}{N}\sum_{i=1}^{N}\nabla_{\theta}\mathcal{W}(x_i ; \theta^{(T)}) . \]
Here, $\mathop{write}$ is modelled by encoder and $\mathop{read}$ is modelled by decoder.</p>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#paper_summary-ref" title="View posts tagged with &quot;paper_summary&quot;">paper_summary</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/post/fixing-the-nans-in-svdbackward-problem/">NaNs in SVD.backward()</a></div>
      <p class="meta"><a class="permalink" href="/post/fixing-the-nans-in-svdbackward-problem/">&#9679;</a> 08 Sep 2019</p>
      <div>
        <p>Typically, in the case of datasets such as MNIST, CIFAR-10; we have categorical labels. When the number of samples is less than the number of categories, this means that the kernel matrix of such dataset will be severly rank deficient; no matter whatever the function map is used. In RKMs, we can easily hedge that problem because we add many kernel matrices togather. Hence, the resulting matrix could be made well-conditioned, since the data in $\mathcal{X}$ is typically distinct.</p>

<p>PyTorch’s grad through SVD algorithm on CUDA is unstable. The same procedure works pretty well on CPU but throws error when used on GPU. Find out the exact algorithm.</p>


        </div>
             
	
    	<span class="label label-default"><a href="/tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/post/intuition-behind-generative-rkm/">Generative RKM</a></div>
      <p class="meta"><a class="permalink" href="/post/intuition-behind-generative-rkm/">&#9679;</a> 06 Sep 2019</p>
      <div>
        <p>Extracting feature through feature maps (in our case CNN), and then forming a latent space whose basis is formed by eigenvectors of kernel matrix. We strongly suspect that this encodes a disentangled representation of input data. This is in strong spirit with latent variable models, where we seek to find the indpendent hidden factors that lead to data generation in first place. For further connections, see post on <a href="/post/latent-variable-models/index.html">latent variable models</a>.</p>

<p>With reference to the recent arxiv paper, in the alternating minimization procedure, by updating the network parameters we are essentially trying to get close to 0. Since this is what we should get by substituting $\mathbf{h}_i$ and $\lambda$ into the objective function. In the ideal case, appropriately defined reconstruction errors should also be 0.</p>

<h2 id="links-with-augmented-lagrangianquadratic-penalty-method">Links with Augmented Lagrangian/Quadratic penalty method</h2>
<p>The derivation of $\mathcal{J}_{stab}$ from $\mathcal{J}$ seems possible from the “perturbed problem in penalty method”.</p>


        </div>
             
	
    	<span class="label label-default"><a href="/tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/post/beta-vae-learning-basic-visual-concepts-with-aconstrained-variational-framework/">Beta-VAE - Learning Basic Visual Concepts with a Constrained Variational Framework</a></div>
      <p class="meta"><a class="permalink" href="/post/beta-vae-learning-basic-visual-concepts-with-aconstrained-variational-framework/">&#9679;</a> 06 Sep 2019</p>
      <div>
        <p><a href="https://openreview.net/references/pdf?id=Sy2fzU9gl">Original Paper link</a></p>

<p>They propose $β$-VAE, a deep unsupervised generative approach for disentangled factor learning that can automatically discover the <em>independent latent
factors</em> of variation in unsupervised data (see post on <a href="/post/latent-variable-models/">Latent Variable Models</a>).</p>

<p>They propose
to augment the original VAE framework with a single hyperparameter $β$ that controls the extent
of learning constraints applied to the model. The constraints impose a limit on the capacity of the
latent information channel and an emphasis on learning statistically <em>independent latent factors</em>. This,
when combined with the data log likelihood maximisation objective of generative models, leads to
the model acquiring the most efficient latent representation of the data, which is disentangled if the
data is produced using at least some ground truth factors of variation that are independent.</p>

<p>They develop a new <em>measure of disentanglement</em> and show that β-VAE also significantly outperforms all
our baselines on this measure (ICA, PCA, VAE by Kingma &amp; Ba (2014), DC-IGN by Kulkarni et al.
(2015), and InfoGAN by Chen et al. (2016))</p>

<p><strong>Key Step:</strong> In order
to encourage this disentangling property in the inferred $q_{\phi}(z\vert x)$, we introduce a constraint over it by
trying to match it to a prior $p(z)$ that can both control the capacity of the latent information bottleneck,
and embodies the desiderata of statistical independence mentioned above. This can be achieved if
we set the prior to be an isotropic unit Gaussian ($p(z) = N(0; \mathbf{I})$), hence arriving at the constrained
optimisation problem, where $\epsilon$ specifies the strength of the applied constraint.</p>

<script type="math/tex; mode=display">% <![CDATA[
\underset{φ;θ}{\mathrm{max}}~
\mathbb{E}_{x\sim D} \left[ \mathbb{E}_{q_{φ}(z\vert x)}[\log p_{θ}(x\vert z)] \right]  \quad \text{subject to}\quad D_{KL}(q_{φ}(z\vert x)\Vert p(z)) < \epsilon %]]></script>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#paper_summary-ref" title="View posts tagged with &quot;paper_summary&quot;">paper_summary</a></span>
    


     </article>
     <hr>
  


<ul class="pager">
  
    
      <li class="next"><a href="/page2">Older &rarr;</a></li>
  
</ul>
</section>





</body>
</html>
