<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="icon" type="image/icon" href="favicon.ico">


<title>Home</title>


<script src="https://storage.googleapis.com/montco-stats/javascript/vis-4.17.0/dist/vis.js" type="text/javascript"></script>
<link href="https://storage.googleapis.com/montco-stats/javascript/vis-4.17.0/dist/vis-network.min.css" rel="stylesheet" type="text/css" />

<link rel="stylesheet" href="http://localhost:4000/css/syntax.css">
<link rel="stylesheet" href="http://localhost:4000/css/main.css">
</head>
<body>


<header>
<div id="nav_container">
<nav>

  <ul>
  <h1><a href="">Research Notes</a></h1>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/about/">About</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/tutorials/">Tutorials</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/paper_summary/">Paper Summaries</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/distill/">Distills</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/tags/">Archives</a>
      </li>
    
  </ul>

</nav>
</div>
</header>

<section id="main">
	  
      <article>
      <div class="heading"><a href="/post/fixing-the-nans-in-svdbackward-problem/">Fixing the NaNs in SVD.backward() problem</a></div>
      <p class="meta"><a class="permalink" href="/post/fixing-the-nans-in-svdbackward-problem/">&#9679;</a> 08 Sep 2019</p>
      <div>
        <p>Typically, in the case of datasets such as MNIST, CIFAR-10; we have categorical labels. When the number of samples is less than the number of categories, this means that the kernel matrix of such dataset will be severly rank deficient; no matter whatever the function map is used. In RKMs, we can easily hedge that problem because we add many kernel matrices togather. Hence, the resulting matrix could be made well-conditioned, since the data in $\mathcal{X}$ is typically distinct.</p>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/post/intuition-behind-generative-rkm/">Generative RKM</a></div>
      <p class="meta"><a class="permalink" href="/post/intuition-behind-generative-rkm/">&#9679;</a> 06 Sep 2019</p>
      <div>
        <p>Extracting feature through feature maps (in our case CNN), and then forming a latent space whose basis is formed by eigenvectors of kernel matrix. We strongly suspect that this encodes a disentangled representation of input data. This is in strong spirit with latent variable models, where we seek to find the indpendent hidden factors that lead to data generation in first place. For further connections, see post on <a href="/post/latent-variable-models/index.html">latent variable models</a>.</p>

<p>With reference to the recent arxiv paper, in the alternating minimization procedure, by updating the network parameters we are essentially trying to get close to 0. Since this is what we should get by substituting $\mathbf{h}_i$ and $\lambda$ into the objective function. In the ideal case, appropriately defined reconstruction errors should also be 0.</p>

<h2 id="links-with-augmented-lagrangianquadratic-penalty-method">Links with Augmented Lagrangian/Quadratic penalty method</h2>
<p>The derivation of $\mathcal{J}_{stab}$ from $\mathcal{J}$ seems possible from the “perturbed problem in penalty method”.</p>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/post/beta-vae-learning-basic-visual-concepts-with-aconstrained-variational-framework/">Beta-VAE - Learning Basic Visual Concepts with a Constrained Variational Framework</a></div>
      <p class="meta"><a class="permalink" href="/post/beta-vae-learning-basic-visual-concepts-with-aconstrained-variational-framework/">&#9679;</a> 06 Sep 2019</p>
      <div>
        <p><a href="https://openreview.net/references/pdf?id=Sy2fzU9gl">Original Paper link</a></p>

<p>They propose $β$-VAE, a deep unsupervised generative approach for disentangled factor learning that can automatically discover the <em>independent latent
factors</em> of variation in unsupervised data (see post on <a href="/post/latent-variable-models/">Latent Variable Models</a>).</p>

<p>They propose
to augment the original VAE framework with a single hyperparameter $β$ that controls the extent
of learning constraints applied to the model. The constraints impose a limit on the capacity of the
latent information channel and an emphasis on learning statistically <em>independent latent factors</em>. This,
when combined with the data log likelihood maximisation objective of generative models, leads to
the model acquiring the most efficient latent representation of the data, which is disentangled if the
data is produced using at least some ground truth factors of variation that are independent.</p>

<p>They develop a new <em>measure of disentanglement</em> and show that β-VAE also significantly outperforms all
our baselines on this measure (ICA, PCA, VAE by Kingma &amp; Ba (2014), DC-IGN by Kulkarni et al.
(2015), and InfoGAN by Chen et al. (2016))</p>

<p><strong>Key Step:</strong> In order
to encourage this disentangling property in the inferred $q_{\phi}(z\vert x)$, we introduce a constraint over it by
trying to match it to a prior $p(z)$ that can both control the capacity of the latent information bottleneck,
and embodies the desiderata of statistical independence mentioned above. This can be achieved if
we set the prior to be an isotropic unit Gaussian ($p(z) = N(0; \mathbf{I})$), hence arriving at the constrained
optimisation problem, where $\epsilon$ specifies the strength of the applied constraint.</p>

<script type="math/tex; mode=display">% <![CDATA[
\underset{φ;θ}{\mathrm{max}}~
\mathbb{E}_{x\sim D} \left[ \mathbb{E}_{q_{φ}(z\vert x)}[\log p_{θ}(x\vert z)] \right]  \quad \text{subject to}\quad D_{KL}(q_{φ}(z\vert x)\Vert p(z)) < \epsilon %]]></script>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#paper_summary-ref" title="View posts tagged with &quot;paper_summary&quot;">paper_summary</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/post/latent-space-cartography-lie-et-al-2019/">Latent Space Cartography (Liu et. al, 2019)</a></div>
      <p class="meta"><a class="permalink" href="/post/latent-space-cartography-lie-et-al-2019/">&#9679;</a> 03 Sep 2019</p>
      <div>
        <p><a href="https://idl.cs.washington.edu/files/2019-LatentSpaceCartography-EuroVis.pdf">Paper Link</a></p>

<p>Employes latent-space visualization techniques (t-SNE, UMAP, and PCA) and contributes to methods eastbalishing semantic relationship between latent-space and the generated model. In particular, they contribute</p>
<ol>
  <li>
    <p>a review of domain literature and a characterization of common latent-space interpretation goals and tasks;</p>
  </li>
  <li>
    <p>a visual analysis system that supports these tasks, including novel projection strategies, visual and statistical methods to assess attribute vector uncertainty, and global attribute vector comparison methods; and</p>
  </li>
  <li>
    <p>three case studies demonstrating new insights into diverse data types across multiple domains.</p>
  </li>
</ol>

<p>In the literature, there exists 6 interpretation sub-tasks:</p>

<ol>
  <li>
    <p><strong>View Reconstruction Examples:</strong> A majority of articles on generative models (31/54) present a list of example generation results. These examples serve as qualitative evidence that the models produce compelling, plausible, yet novel outputs unseen in the training data. They also show reconstruction fidelity versus generation diversity, as there is usually a trade-off between the two.</p>
  </li>
  <li>
    <p><strong>View Interpolation Results:</strong> Linear interpolation in a continuous latent space is done by following a path between two points and displaying outputs at sampled locations on the path (usually in steps of equal distance). A number of articles (18/54) adopt this approach. Interpolation sequences naturally show the smoothness of the latent space, and they are important indicators of interpretability. For instance, authors may point out subtle changes in transitions that reflect deeper domain-specific principles</p>
  </li>
  <li>
    <p><strong>Examine Nearest Neighbors:</strong> This task is employed by articles on both word embeddings (10/24) and generative models (6/54), but the emphasis is different. Word embedding articles use
anecdotal nearest neighbors to qualitatively show what the algorithms manage to achieve. Articles on generative models use nearest neighbors from the training set as a comparison, demonstrating
that the algorithms generate novel results indistinguishable from, or better than, the input.</p>
  </li>
  <li>
    <p><strong>Perform Attribute Vector Arithmetic:</strong> An example of this task for word embeddings shows that the vector(king) - vector(man) + vector(woman) produces a vector very close to vector(queen).
This simple algebraic operation demonstrates that pairs of data points sharing a particular relationship have approximately constant vector offsets. We observe this task in both generative (3/54)
and word embedding (10/24) models.</p>
  </li>
  <li>
    <p><strong>Compare Similarities:</strong> This task is popular for benchmarks
of word embeddings (13/24), testing how well the cosine similarity
between word vectors matches ratings from human annotators.</p>
  </li>
  <li>
    <p><strong>Visualize Distribution:</strong> Researchers visualize latent spaces
using various means described in §3. This task is common to both
generative (20/54) and word embedding (7/24) settings.</p>
  </li>
</ol>


        </div>
             
	
    	<span class="label label-default"><a href="/tags#paper_summary-ref" title="View posts tagged with &quot;paper_summary&quot;">paper_summary</a></span>
    


     </article>
     <hr>
  


<ul class="pager">
  
    
      <li class="next"><a href="/page2">Older &rarr;</a></li>
  
</ul>
</section>





</body>
</html>
