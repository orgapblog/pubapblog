<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>A small and simple site I made to test out Jekyll.</description>		
		<link>http://localhost:4000</link>
		<atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Fixing the NaNs in SVD.backward() problem</title>
				<description>&lt;p&gt;Typically, in the case of datasets such as MNIST, CIFAR-10; we have categorical labels. When the number of samples is less than the number of categories, this means that the kernel matrix of such dataset will be severly rank deficient; no matter whatever the function map is used. In RKMs, we can easily hedge that problem because we add many kernel matrices togather. Hence, the resulting matrix could be made well-conditioned, since the data in $\mathcal{X}$ is typically distinct.&lt;/p&gt;
</description>
				<pubDate>Sun, 08 Sep 2019 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/post/fixing-the-nans-in-svdbackward-problem/</link>
				<guid isPermaLink="true">http://localhost:4000/post/fixing-the-nans-in-svdbackward-problem/</guid>
			</item>
		
			<item>
				<title>Intuition behind Generative RKM</title>
				<description>&lt;p&gt;Extracting feature through feature maps (in our case CNN), and then forming a latent space whose basis is formed by eigenvectors of kernel matrix. We strongly suspect that this encodes a disentangled representation of input data. This is in strong spirit with latent variable models, where we seek to find the indpendent hidden factors that lead to data generation in first place. For further connections, see post on &lt;a href=&quot;/post/latent-variable-models/index.html&quot;&gt;latent variable models&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Fri, 06 Sep 2019 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/post/intuition-behind-generative-rkm/</link>
				<guid isPermaLink="true">http://localhost:4000/post/intuition-behind-generative-rkm/</guid>
			</item>
		
			<item>
				<title>Beta-VAE - Learning Basic Visual Concepts with a Constrained Variational Framework</title>
				<description>&lt;p&gt;&lt;a href=&quot;https://openreview.net/references/pdf?id=Sy2fzU9gl&quot;&gt;Original Paper link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;They propose $β$-VAE, a deep unsupervised generative approach for disentangled factor learning that can automatically discover the &lt;em&gt;independent latent
factors&lt;/em&gt; of variation in unsupervised data (see post on &lt;a href=&quot;/post/latent-variable-models/&quot;&gt;Latent Variable Models&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;They propose
to augment the original VAE framework with a single hyperparameter $β$ that controls the extent
of learning constraints applied to the model. The constraints impose a limit on the capacity of the
latent information channel and an emphasis on learning statistically &lt;em&gt;independent latent factors&lt;/em&gt;. This,
when combined with the data log likelihood maximisation objective of generative models, leads to
the model acquiring the most efficient latent representation of the data, which is disentangled if the
data is produced using at least some ground truth factors of variation that are independent.&lt;/p&gt;

&lt;p&gt;They develop a new &lt;em&gt;measure of disentanglement&lt;/em&gt; and show that β-VAE also significantly outperforms all
our baselines on this measure (ICA, PCA, VAE by Kingma &amp;amp; Ba (2014), DC-IGN by Kulkarni et al.
(2015), and InfoGAN by Chen et al. (2016))&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key Step:&lt;/strong&gt; In order
to encourage this disentangling property in the inferred $q_{\phi}(z\vert x)$, we introduce a constraint over it by
trying to match it to a prior $p(z)$ that can both control the capacity of the latent information bottleneck,
and embodies the desiderata of statistical independence mentioned above. This can be achieved if
we set the prior to be an isotropic unit Gaussian ($p(z) = N(0; \mathbf{I})$), hence arriving at the constrained
optimisation problem, where $\epsilon$ specifies the strength of the applied constraint.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\underset{φ;θ}{\mathrm{max}}~
\mathbb{E}_{x\sim D} \left[ \mathbb{E}_{q_{φ}(z\vert x)}[\log p_{θ}(x\vert z)] \right]  \quad \text{subject to}\quad D_{KL}(q_{φ}(z\vert x)\Vert p(z)) &lt; \epsilon %]]&gt;&lt;/script&gt;
</description>
				<pubDate>Fri, 06 Sep 2019 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/post/beta-vae-learning-basic-visual-concepts-with-aconstrained-variational-framework/</link>
				<guid isPermaLink="true">http://localhost:4000/post/beta-vae-learning-basic-visual-concepts-with-aconstrained-variational-framework/</guid>
			</item>
		
			<item>
				<title>Latent Space Cartography (Liu et. al, 2019)</title>
				<description>&lt;p&gt;&lt;a href=&quot;https://idl.cs.washington.edu/files/2019-LatentSpaceCartography-EuroVis.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Employes latent-space visualization techniques (t-SNE, UMAP, and PCA) and contributes to methods eastbalishing semantic relationship between latent-space and the generated model. In particular, they contribute&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;a review of domain literature and a characterization of common latent-space interpretation goals and tasks;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;a visual analysis system that supports these tasks, including novel projection strategies, visual and statistical methods to assess attribute vector uncertainty, and global attribute vector comparison methods; and&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;three case studies demonstrating new insights into diverse data types across multiple domains.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the literature, there exists 6 interpretation sub-tasks:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;View Reconstruction Examples:&lt;/strong&gt; A majority of articles on generative models (31/54) present a list of example generation results. These examples serve as qualitative evidence that the models produce compelling, plausible, yet novel outputs unseen in the training data. They also show reconstruction fidelity versus generation diversity, as there is usually a trade-off between the two.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;View Interpolation Results:&lt;/strong&gt; Linear interpolation in a continuous latent space is done by following a path between two points and displaying outputs at sampled locations on the path (usually in steps of equal distance). A number of articles (18/54) adopt this approach. Interpolation sequences naturally show the smoothness of the latent space, and they are important indicators of interpretability. For instance, authors may point out subtle changes in transitions that reflect deeper domain-specific principles&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Examine Nearest Neighbors:&lt;/strong&gt; This task is employed by articles on both word embeddings (10/24) and generative models (6/54), but the emphasis is different. Word embedding articles use
anecdotal nearest neighbors to qualitatively show what the algorithms manage to achieve. Articles on generative models use nearest neighbors from the training set as a comparison, demonstrating
that the algorithms generate novel results indistinguishable from, or better than, the input.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Perform Attribute Vector Arithmetic:&lt;/strong&gt; An example of this task for word embeddings shows that the vector(king) - vector(man) + vector(woman) produces a vector very close to vector(queen).
This simple algebraic operation demonstrates that pairs of data points sharing a particular relationship have approximately constant vector offsets. We observe this task in both generative (3/54)
and word embedding (10/24) models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Compare Similarities:&lt;/strong&gt; This task is popular for benchmarks
of word embeddings (13/24), testing how well the cosine similarity
between word vectors matches ratings from human annotators.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Visualize Distribution:&lt;/strong&gt; Researchers visualize latent spaces
using various means described in §3. This task is common to both
generative (20/54) and word embedding (7/24) settings.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Tue, 03 Sep 2019 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/post/latent-space-cartography-lie-et-al-2019/</link>
				<guid isPermaLink="true">http://localhost:4000/post/latent-space-cartography-lie-et-al-2019/</guid>
			</item>
		
			<item>
				<title>Energy-based learning</title>
				<description>&lt;h2 id=&quot;ising-model&quot;&gt;Ising model&lt;/h2&gt;

&lt;p&gt;Ref. pg 400 of book Information theory, Inference, and Learning Algorithms.&lt;/p&gt;

&lt;p&gt;Ising models are important first as models of magnetic systems that have
a phase transition. The theory of universality in statistical physics shows that
all systems with the same dimension (here, two), and the same symmetries,
have equivalent critical properties, i.e., the scaling laws shown by their phase
transitions are identical. So by studying Ising models we can find out not only
about magnetic phase transitions but also about phase transitions in many
other systems.&lt;/p&gt;

&lt;p&gt;General form of Ising model is known as ‘spin glasses’ to physicists, and as ‘Hopfield networks’ or ‘Boltzmann machines’ to the neural network community. In some of these models, all spins are declared to be neighbours of each other, in which case physicists call the system an ‘infinite-range’ spin glass, and networkers call it a ‘fully connected’ network.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf&quot;&gt;A Tutorial on Energy-based Learning, Yann LeCunn, 2004&lt;/a&gt;&lt;/p&gt;

</description>
				<pubDate>Tue, 03 Sep 2019 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/post/energy-based-learning/</link>
				<guid isPermaLink="true">http://localhost:4000/post/energy-based-learning/</guid>
			</item>
		
			<item>
				<title>Latent variable models</title>
				<description>&lt;p&gt;Many statistical models that are generative models (that is, models that specify a full probability density over all variables in the situation) make use of latent variables to describe a probability distribution over observables.&lt;/p&gt;

&lt;p&gt;Some examples of latent variable models are mixture models, which model the observables as coming from a superposed mixture of simple probability distributions (the latent variables are the unknown class labels of the examples); hidden Markov models (Rabiner and Juang, 1986; Durbin et al., 1998); Probabilistic PCA; Gaussian-Process Latent variable model (GP-LVM, &lt;a href=&quot;https://papers.nips.cc/paper/2540-gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data.pdf&quot;&gt;Lawrence 2005&lt;/a&gt;) and factor analysis. More recent ones that are popularized by ML community are RBMs, VAEs and GANs.&lt;/p&gt;

&lt;p&gt;The decoding problem for error-correcting codes can also be viewed in terms of a latent variable model - figure below. In that case, the encoding matrix $G$ is normally known in advance. In latent variable modelling, the parameters equivalent to $G$ are usually not known, and must be inferred from the data along with the latent variables $s$. In the figure, the $K$ latent variables are the independent source bits $s_1, \dots , s_K$; these give rise to the observables via the generator matrix $G$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/photos/latent_var.PNG&quot; alt=&quot;&quot; style=&quot;width: 250px; height: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Usually, the latent variables have a simple distribution, often a separable distribution. Thus when we fit a latent variable model, we are finding a description of the data in terms of ‘independent components’. The ‘&lt;em&gt;independent component analysis&lt;/em&gt;’ algorithm corresponds to perhaps the simplest possible latent variable model with continuous latent variables.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;independent-component-analysis&quot;&gt;Independent Component Analysis&lt;/h3&gt;

&lt;p&gt;Some references: &lt;a href=&quot;/post/latent-variable-models/&quot;&gt;tutorial&lt;/a&gt; and  Ch. 34 in David Mckay - `Information theory, Inference and Learning Algorithms’.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;MLE of ICA has the same form as Normalizing flows. Perhaps, NFs could be exploited to computed ICA ?!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Is there a connection between ICA and KPCA?? The eigenvectors are independent, but what can we say about the non-gaussianity?!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Fri, 30 Aug 2019 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/post/latent-variable-models/</link>
				<guid isPermaLink="true">http://localhost:4000/post/latent-variable-models/</guid>
			</item>
		
			<item>
				<title>Jekyll blog with OAuth</title>
				<description>&lt;p&gt;Refer these blogs &lt;a href=&quot;https://ben.balter.com/jekyll-auth/&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://blog.heroku.com/jekyll-on-heroku&quot;&gt;2&lt;/a&gt;.
Combine the ideas from both.&lt;/p&gt;
</description>
				<pubDate>Wed, 28 Aug 2019 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/post/jekyll-blog-with-oauth/</link>
				<guid isPermaLink="true">http://localhost:4000/post/jekyll-blog-with-oauth/</guid>
			</item>
		
			<item>
				<title>Variational Inference of Normalizing Flows (Rezende &amp; Shakir, 2015)</title>
				<description>&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v37/rezende15.pdf&quot;&gt;Original paper link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;normalizing-flows&quot;&gt;Normalizing Flows&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;\photos\shakir_danilo_slide.png&quot; alt=&quot;&quot; style=&quot;width: 600px; height: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key idea&lt;/strong&gt;: Assume a simple probability distribution, take a sample from it and then &lt;strong&gt;&lt;em&gt;transform&lt;/em&gt;&lt;/strong&gt; that sample. This is equivalent to change of variables in probability distributions and, if the transformation meets some mild conditions, can result in a very complex pdf of the transformed variable. The formalism of normalizing flows now gives us a systematic way of specifying the approximate posterior distributions $q(z\vert x)$ required for variational inference. (see &lt;a href=&quot;/post/inf_prob_models/&quot;&gt;Inference in probabilistic models&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Example: If $f$ is monotonically increasing or monotonically decreasing function, such that $y = f(x)$, $x \sim p_{X}$, then we have:
\[ p_{Y}(y)= \frac{d}{dy}\mathbb{P}(Y\leq y)= \frac{d}{dy}\mathbb{P}(f(X)\leq y)=\frac{d}{dy}\mathbb{P}(X\leq f^{-1}(y))=p_{X}(f^{-1}(y))\frac{df^{-1}(y)}{dy}=p_{X}(x)\frac{df^{-1}(y)}{dy},\]
\[ p_{Y}(y)= \frac{d}{dy}\mathbb{P}(Y\leq y)= \frac{d}{dy}\mathbb{P}(f(X)\leq y)=\frac{d}{dy}\mathbb{P}(X\geq f^{-1}(y))=-p_{X}(f^{-1}(y))\frac{df^{-1}(y)}{dy}=-p_{X}(x)\frac{df^{-1}(y)}{dy},\]
respectively.&lt;/p&gt;

&lt;p&gt;Similarly for multivariate invertible mappings $f:\mathbb{R}^{m}\mapsto \mathbb{R}^{m}$ and $X\sim p_{X}$, $Y=f(X)$, we get:&lt;/p&gt;

&lt;p&gt;\[\boxed{ p_{Y}(y)=p_{X}(x)\left\lvert \operatorname{det}\frac{df^{-1}(y)}{dy}\right\rvert =p_{X}(x)\left\lvert \operatorname{det}\frac{df(x)}{dy}\right\rvert^{-1} },\qquad complexity \rightarrow O(m^{3})\]
where the last equality exploits the &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse_function_theorem&quot;&gt;property of jacobian of inverse functions&lt;/a&gt; (PS: invertibility of $f(\cdot)$ validates the use of this property) and $[ \mathrm{det}(A^{-1}) =\mathrm{det}(A)^{-1} ]$.&lt;/p&gt;

&lt;p&gt;NFs are typically used to parametrise the approximate posterior $q(z\vert x)$ but can also be applied for the likelihood function. We can apply a series of mappings  $f_{k},~ k= [ 1,\dots,K ]$ and obtain a normalizing flow:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{z}_K = f_K \circ \dots \circ f_1 (\mathbf{z}_0), \quad \mathbf{z}_0 \sim q_0(\mathbf{z}_0),&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{z}_K \sim q_K(\mathbf{z}_K) = q_0(\mathbf{z}_0) \prod_{k=1}^K
  \left|
    \mathrm{det} \frac{
      \partial f_k
    }{
      \partial \mathbf{z}_{k-1}\
    }
  \right| ^{-1}, \quad or&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ln q_K (\mathbf{z}_K) = \ln q_0(\mathbf{z}_0) - \sum_{k=1}^{K} \ln \mathrm{det} \left|
     \frac{
      \partial f_k
    }{
      \partial \mathbf{z}_{k-1}\
    }
  \right|.&lt;/script&gt;

&lt;p&gt;This series of transformations can transform a simple probability distribution (e.g. Gaussian) into a complicated multi-modal one. Note that after every mapping ($f_1, f_2, \dots $), the det-Jac has to be computed to ensure that the output is also a distribution. To be of practical use, however, we can consider only transformations whose determinants of Jacobians are easy to compute. The original paper considered two simple family of transformations, named planar and radial flows.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;planar-flow&quot;&gt;Planar Flow&lt;/h2&gt;
&lt;p&gt;\[ f(\mathbf{z}) = \mathbf{z} + \mathbf{u} h(\mathbf{w}^{\top} \mathbf{z} + b) \]
with $\mathbf{U}, \mathbf{W}\in \mathbb{R}^{d}$ and $b\in \mathbb{R}$ and $h$ an element-wise non-linearity. Let $\psi (\mathbf{z}) = h’ (\mathbf{w}^T \mathbf{z} + b) \mathbf{w}$. Then the determinant can be easily computed as&lt;/p&gt;

&lt;p&gt;\[ \left| \mathrm{det} \frac{\partial f}{\partial \mathbf{z}} \right| =
  \left| 1 + \mathbf{u}^{\top} \psi( \mathbf{z} ) \right| = \left| 1 + \mathbf{u}^{\top} h’ (\mathbf{w}^T \mathbf{z} + b) \mathbf{w} \right| .  \]
Since $0\leq h’ \leq 1$ for $ h(x) = \tanh (x) $, we require $ \mathbf{u}^{\top}\mathbf{w} \geq -1$ for invertibility of such flows. We can think of planar flows as slicing the $\mathbf{z}$-space with straight lines (or hyperplanes), where each line contracts or expands the space around it, see figure 1 (in paper).&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;radial-flow&quot;&gt;Radial Flow&lt;/h2&gt;
&lt;p&gt;\[  f(\mathbf{z}) = \mathbf{z} + \beta h(\alpha, r)(\mathbf{z} - \mathbf{z}_0),\]&lt;/p&gt;

&lt;p&gt;with &lt;script type=&quot;math/tex&quot;&gt;r = \Vert\mathbf{z} - \mathbf{z}_0 \Vert_2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;h(\alpha, r) = \frac{1}{\alpha + r}&lt;/script&gt; and parameters &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}_0 \in \mathbb{R}^d, \alpha \in \mathbb{R}_+&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta \in \mathbb{R}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Similarly to planar flows, radial flows introduce spheres in the $z$-space, which either contract or expand the space inside the sphere, see figure 1 (in paper).&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;algorithmic-steps&quot;&gt;Algorithmic steps&lt;/h3&gt;
&lt;p&gt;Suppose the unknown target distribution is specified using energy functions $U(z)$, $p(z) = \frac{1}{\mathcal{Z}} \exp(-U(z))$, where $\mathcal{Z}$ is the unknown partition function (normalization constant); that is, $p(z) \propto \exp({-U(z)})$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Steps:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Generate random samples from initial distribution $z_{0} \sim q_0 (z) = \mathcal{N}(z; \mu, \sigma^2 I)$.
Here $\mu$ and $\sigma$ can either be fixed (such as standard Normal distribution) or can be estimated as follows:
Draw auxillary random variable $\epsilon$ from standard normal distribution $\epsilon \sim \mathcal{N}(0, I)$ and apply linear normalizing flow transformation $f(\epsilon) = \mu + \sigma \epsilon$, re-parameterizing $\sigma = e^{(\frac{1}{2}*\log(var))}$ to ensure $\sigma &amp;gt; 0$, then jointly optimize {$\mu$, $var$} together with the other normalizing flow parameters (see below).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Transform the initial samples $z_0$ through $K$ &lt;em&gt;Normalizing Flow&lt;/em&gt; transforms, from which we obtain the transformed approximate distribution $q_{K}(z)$,
  $\log q_{K}(z) = \log q_0 (z) - \sum_{k=1}^K \log (\mathrm{det} |J_k|)$
where $J_k$ is the Jacobian of the $k$-th (invertible) normalizing flow transform.
E.g. for planar flows,
  $\log q_{K}(z) = \log q_0 (z) - \sum_{k=1}^K \log |1 + u_k^{\top} \psi_k(z_{k-1})|$
where each flow includes model parameters $\lambda = \{ w, u, b \}$.&lt;/li&gt;
  &lt;li&gt;Jointly optimize all model parameters by minimizing &lt;em&gt;KL-Divergence&lt;/em&gt; between the approximate distribution $q_{K}(z)$
and the true distribution $p(z)$.
  &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
           loss = KL[q_{K}(z)||p(z)] &amp; = \mathbb{E}_{z_K \sim q_{K}(z)} \left[ \log q_K(z_K) - \log p(z_K) \right]\\
                          &amp;= \mathbb{E}_{z_K \sim q_{K}(z)} \left[ \left(\log q_0(z_0) - \sum_k \log (\mathrm{det} |J_k|)\right) - \left(-U(z_K) - \log(\mathcal{Z})\right) \right]\\
                         &amp;= \mathbb{E}_{z_0 \sim q_0(z)} \left[ \log q_0(z_0) - \sum_k \log \left(\mathrm{det} |J_k|\right) + U(f_1(f_2(\dots f_K(z_0)))) + \log(\mathcal{Z}) \right]
  \end{align*} %]]&gt;&lt;/script&gt;&lt;br /&gt;
Here the partition function $\mathcal{Z}$ is independent of $z_0$ and model parameters, so we can ignore it for the optimization
  $loss = \mathbb{E}_{z_0 \sim q_0(z)} \left[ \log q_0(z_0) - \sum_k \log (\mathrm{det} |J_k|) + U(z_K) \right]$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The expectation could be approximated by Monte Carlo sampling, the mini-batch average here could be considered as the Monte Carlo Sampling Expectation.&lt;/p&gt;

&lt;h3 id=&quot;ex-density-estimation-generative-modelling&quot;&gt;Ex: Density estimation (Generative modelling)&lt;/h3&gt;
&lt;p&gt;$p_X:$ data distribution, $p_Y:$ prior over latent variable, $p_{f^{-1}(Y)}:$ push-forward density of $f^{-1}(Y)$, i.e the generative model&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} 
 D_{KL}(p_X \Vert p_{f^{-1}(Y)}) &amp; = \mathbb{E}_{x\sim p_{X}}\left[ \log p_X (x) - \log p_{f^{-1}(Y)} (x) \right] \\ 
                                 &amp; = \mathbb{E}_{x\sim p_{X}}\left[ \log p_X (x) - \log p_{X} (x) \right] \\
                                 &amp; = \mathbb{E}_{x\sim p_{X}}\left[ \log p_X (x) - \log p_{Y} (f(x))\left| \mathrm{det} \frac{\partial f(x)}{\partial x} \right|\right]\\
                                 &amp; = \mathbb{E}_{x\sim p_{X}}\left[ \log p_X (x) - \log p_{Y} (f(x)) - \log \left| \mathrm{det} \frac{\partial f(x)}{\partial x} \right| \right]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;See &lt;a href=&quot;https://arxiv.org/abs/1511.01844&quot;&gt;A note on the evaluation of generative models&lt;/a&gt; for a thought-provoking discussion about how high log-likelihood is neither sufficient nor necessary to generate “plausible” images. Still, it’s better than nothing and in practice a useful diagnostic tool.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There’s a connection between Normalizing Flows and GANs via encoder-decoder GAN architectures that learn the inverse of the generator (ALI / BiGAN). Since there is a separate encoder trying to recover $u=G^{−1}(x)$ such that $x=G(u)$, the generator can be thought of as a flow for the simple uniform distribution. However, we don’t know how to compute the amount of volume expansion/contraction w.r.t. $x$, so we cannot recover density from GANs. However, it’s probably not entirely unreasonable to model the log-det-jacobian numerically or enforce some kind of linear-time Jacobian by construction.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Tue, 27 Aug 2019 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/post/checksum/</link>
				<guid isPermaLink="true">http://localhost:4000/post/checksum/</guid>
			</item>
		
			<item>
				<title>Optimal Stopping Problem</title>
				<description>&lt;p&gt;There are a lot of everyday situations where people make decisions one after the other, and what is decided earlier affects the choices later on. Most people probably go by some gut feeling about when it’s time to stop and select the next best option and settle down.&lt;/p&gt;

&lt;p&gt;In this article we will examine the “Optimal Stopping Problem,” a mathematical puzzle that can manifest in many different real world examples. For instance:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When should a hiring manager stop interviewing people for a new job position? &lt;br /&gt;
This is the classical “secretary problem” within the field of study of mathematics: probability-optimization. This problem in it’s simplest form has following features:
    &lt;ul&gt;
      &lt;li&gt;There is one secretarial position available.&lt;/li&gt;
      &lt;li&gt;The number of applicants is $n$.&lt;/li&gt;
      &lt;li&gt;The applicants are interviewed sequentially in random order, each order being equally likely.&lt;/li&gt;
      &lt;li&gt;After each interview the hiring manager must decide to reject or hire the applicant.&lt;/li&gt;
      &lt;li&gt;An applicant once rejected cannot later be recalled.&lt;/li&gt;
      &lt;li&gt;Once the hiring manager decides to hire an applicant his/her search is over and there is no need to interview any more candidates.&lt;br /&gt;
This basic problem has a remarkably simple solution and was outlined in the basic paper by  Gilbert and Mosteller ( 1966 ), with elegant derivations and extensions in a number of important directions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We have a house and we wish to sell it. Each day we get a new offer for the house, which could be higher or lower than the previous offer; however, each day the house is on the market we must pay for advertising. We wish to maximise the amount you earn by choosing a stopping rule. When should we stop looking at potential buyers and finally settle the deal?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At some point in a lot of peoples lives, we wonder when we should stop dating and finally settle down with the right partner. Most people probably go by some gut feeling about when it’s time to stop. Editor’s Note: Of course, for mathematicians this is a purely theoretical example.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Suppose we are a doctor and patients arrive sequentially at our clinic and must be treated immediately by one of the treatments. It is assumed that response from treatment is immediate so that the effectiveness of the treatment that the present patient receives is known before the next patient is treated. It is not known precisely which one of the treatments is best, but we must decide which treatment to give each patient, keeping in mind that our goal is to cure as many patients as possible. This may require us to give a patient a treatment which is not the one that looks best at the present time in order to gain information that may be of use to future patients. When should we &lt;strong&gt;stop evaluating&lt;/strong&gt; more treatments and finally select the best one?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are many different solutions to the problems listed above, as the strategy we choose will depend on our objective. Do we want to ensure that the option we select is better than average? What if we just want to be sure that we do not pick the worst option? In this article our goal will be to “&lt;strong&gt;minimize the risk&lt;/strong&gt;” of missing the best option. In other words, we want to “maximize the chances” (i.e. probability) of selecting the best option.&lt;/p&gt;

&lt;h3 id=&quot;the-prelude&quot;&gt;The Prelude&lt;/h3&gt;

&lt;p&gt;The general form of our strategy will be to “gather information” when we are presented with our first options. We know that we will not choose any of these; however, we will examine them in order to know “how good” we should expect our options to be. After a certain point, once we are done gathering information, we will choose the first option that was better than all the ones we have seen before. After some thought, we can see that this strategy makes some intuitive sense. We don’t want to choose an option too early since there would be a high likelihood a better option would come later. But when should we stop gathering information? While we don’t want to choose an option too early, we certainly don’t want to gather information for too long and risk the best option passing us by.&lt;/p&gt;

&lt;p&gt;This type of problem is called the “&lt;strong&gt;Optimal Stopping Problem&lt;/strong&gt;”, which mathematicians and computer scientists have researched for many years.&lt;/p&gt;

&lt;h3 id=&quot;a-real-life-example&quot;&gt;A Real-life Example&lt;/h3&gt;

&lt;p&gt;Suppose we want to sell our house. We have a pool of n potential buyers (we number them from $1$ to $n$), who we meet one after the other. After a potential buyer has made their offer, we must decline or accept their offer. If we decide to accept their offer, we can no longer sell the house to anyone else and if we reject an offer the buyer will take their business elsewhere, and we will not be able to change our minds later on. For simplicity’s sake, we will assume there is no advertising cost (i.e. we do not have to pay to keep our house on the market).&lt;/p&gt;

&lt;p&gt;Among our pool of $n$ people, there’s at least one who gives us the highest value for our property. We will call that person  $m$  (where  $m$  ranges from  $1$  to  $n$) – it’s who we’d ideally want to end up making deal with.&lt;/p&gt;

&lt;p&gt;Our strategy is to discuss the deal with  $m-1$  of the $n$ people and then settle with the next best person who is better. (The reason for doing the calculations with  $m-1$  is that  m is the minimum number of potential buyers that we will encounter; there are  $m-1$  who give us the data we need before we start searching, and the $m^{th}$ is the first one who can be accepted.)&lt;/p&gt;

&lt;p&gt;Given a pool of $n$ potential buyers, let $P(m; n)$ represent the probability that we will choose the best offer if we use the strategy discussed earlier. I.e. If there are $n$ buyers and we “gather information” during the first $m-1$ offers and pick the first offer that is better than the current best, the probability that we will select the best possible offer is $P(m; n)$.&lt;/p&gt;

&lt;p&gt;Notice immediately that $P(1; n) = \dfrac{1}{n}$ and $P(n; n) = \dfrac{1}{n}$. These two cases essentially give no choice, meaning, it is just like the probability of picking one person at random out of total number of people ($n$) and hence the probability is $\dfrac{1}{n}$. But interesting things happen ‘&lt;strong&gt;&lt;em&gt;in between&lt;/em&gt;&lt;/strong&gt;’ these two extremes. As we see more people , we already have a notion of the “best so far,” and the next person we see that beats the current “best so far” is more likely to be the best person in the group for us. Unfortunately, it works the other way around as well. As we continue to see more people, the likelihood of having already passed the best so far increases, thereby decreasing our chances at finding the optimal person.&lt;/p&gt;

&lt;p&gt;We want to find the optimal place to stop – a sort of middle ground that maximizes our chances of choosing the optimal person.&lt;/p&gt;

&lt;p&gt;To do this, we want to maximize $P(m; n)$. In the process of doing this, we want to make sure our solution generalizes. A good way to generalize is to find the right ratio $\dfrac{m}{n}$, the percentage of our population that we should analyze before making a decision.&lt;/p&gt;

&lt;h4 id=&quot;mathematical-formalism&quot;&gt;Mathematical Formalism&lt;/h4&gt;
&lt;p&gt;Our first step here is to get an equation for $P(m; n)$.&lt;/p&gt;

&lt;p&gt;Let’s say we have collected information about $m - 1$ potential buyers and now are looking at the $k^{th}$ person in the sequence, where  $m-1 \leq k \leq n$.&lt;/p&gt;

&lt;p&gt;Now let’s break down some things that are going on here:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We know that this  $k^{th}$  person has a $\dfrac{1}{n}$ probability of being the best buyer.&lt;/li&gt;
  &lt;li&gt;We also know that in order to be considering the $k^{th}$ buyer in this sequence, the highest ranking person thus far must be in the group of $m - 1$ people out of the $k - 1$ people we had previously inspected and rejected (otherwise we would have stopped at someone before the $k^{th}$ person).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And the probability of the highest ranked person thus far being in that group of $m - 1$ people out of the $k - 1$ people inspected is:
\[ \dfrac{m - 1}{k - 1}. \]&lt;/p&gt;

&lt;p&gt;And therefore, the overall probability of finding the best potential partner this time is:
\[ (\dfrac{m - 1}{k - 1}).(\dfrac{1}{n}) = \dfrac{m - 1}{n(k - 1)}\]&lt;/p&gt;

&lt;p&gt;Note: In the following section we will use summation notation ($ \sum $ symbol), which works as follows: Suppose we have a sequence of numbers $x_1, x_2, …, x_n$, then, by definition, $\sum_{i=1}^{n} x_i = x_1 + x_2 + x_3 + … + x_n$. Thus $\sum_{i=1}^{n} 1 = n$ and $\sum_{i=1}^{3} x_i = x_1 + x_2 + x_3$. We use this notation because it is shorter than writing out each term in the sum.&lt;/p&gt;

&lt;p&gt;Now to determine $ P(m; n) $, we must remember that $ k $ can take on any value from $ m $ to $ n $. As a consequence, we must sum these probabilities for any value of $ k $ (note the range of $ k $ above). A compact way to express this is as follows:
\[P(m; n) =\sum_{k=m}^{n} \dfrac{m-1}{n(k - 1)}= \dfrac{m-1}{n}\sum_{k=m}^{n}\dfrac{1}{k-1}. \tag{1} \label{1}\]&lt;/p&gt;

&lt;p&gt;At this point, it is possible to determine the maximum value using methods of optimization one learns in calculus, but this can get nasty pretty quickly. We will lay out a more elegant (albeit more mathematically involved) approach in the following section.&lt;/p&gt;

&lt;h4 id=&quot;to-get-more-technical&quot;&gt;To Get More Technical&lt;/h4&gt;
&lt;p&gt;Given a number $n$, we want to maximize the value of $P(m;n)$. Hence, the optimal value for $ m $ will be such that:
       \[P(m - 1; n) &amp;lt; P(m; n) \text{ and } P(m + 1; n) &amp;lt; P(m;n) \tag{2} \label{2}\]&lt;/p&gt;

&lt;p&gt;Author’s Note: If you want to be very pedantic, you could ask what happens if there are two “best” values of $ m $, with one of those strict inequality signs replaced by a partial inequality. It turns out that the only time when equality is possible is when $ n=2 $, which is not very interesting anyway.&lt;/p&gt;

&lt;p&gt;Let’s break this down part by part. From the first inequality in (\ref{2}) and using (\ref{1}):
\[P(m - 1; n) &amp;lt; P (m; n)\]&lt;/p&gt;

&lt;p&gt;\[\dfrac{m-2}{n}\sum_{k=m-1}^{n} \dfrac{1}{k - 1}&amp;lt;\dfrac{m-1}{n} \sum_{k=m}^{n}\dfrac{1}{k-1}\]&lt;/p&gt;

&lt;p&gt;\[(m-2)\sum_{k=m-1}^{n} \dfrac{1}{k - 1}&amp;lt;(m-1) \sum_{k=m}^{n}\dfrac{1}{k-1}.\]&lt;/p&gt;

&lt;p&gt;Rewriting the left hand side:&lt;/p&gt;

&lt;p&gt;\[(m-2)(\dfrac{1}{m-2})+\sum_{k=m}^{n} \dfrac{1}{k - 1}&amp;lt;(m-1) \sum_{k=m}^{n}\dfrac{1}{k-1}.\]&lt;/p&gt;

&lt;p&gt;After algebraic simplification, we get:
    \[1&amp;lt;\sum_{k=m}^{n}\dfrac{1}{k-1}\tag{3} \label{3}\]&lt;/p&gt;

&lt;p&gt;Let’s now do the same thing for the other inequality from (\ref{2}):&lt;/p&gt;

&lt;p&gt;\[P (m; n) &amp;gt; P (m + 1; n)\]&lt;/p&gt;

&lt;p&gt;\[(m-1)\sum_{k=m}^{n}\dfrac{1}{k-1}&amp;gt;m\sum_{k=m+1}^{n}\dfrac{1}{k-1}\]&lt;/p&gt;

&lt;p&gt;Performing a similar calculations to what we did with the last inequality, we get:&lt;/p&gt;

&lt;p&gt;\[(m-1)(\dfrac{1}{m-1}+\sum_{k=m+1}^{n}\dfrac{1}{k-1})&amp;gt;m\sum_{k=m+1}^{n}\dfrac{1}{k-1}\]&lt;/p&gt;

&lt;p&gt;\[(1+(m-1)\sum_{k=m+1}^{n}\dfrac{1}{k-1})&amp;gt;m\sum_{k=m+1}^{n}\dfrac{1}{k-1}\]&lt;/p&gt;

&lt;p&gt;\[1&amp;gt;\sum_{k=m+1}^{n}\dfrac{1}{k-1}\tag{4}\label{4}\]&lt;/p&gt;

&lt;p&gt;Let’s remember that our goal here is to find an appropriate approximation for the ratio $ \dfrac{m}{n} $ for the best $ m  $.&lt;/p&gt;

&lt;p&gt;One fact that is going to help us immensely in this calculation is the approximation of the partial sum of a harmonic series. Mathematically, the approximation is:&lt;/p&gt;

&lt;p&gt;\[\sum_{k=1}^{x}\dfrac{1}{k}\approx\ln(x)+c \]&lt;/p&gt;

&lt;p&gt;where $ c $ is the Euler-Mascheroni constant.&lt;/p&gt;

&lt;p&gt;Now this approximation greatly helps us simplify inequalities (\ref{3}) and (\ref{4}).&lt;/p&gt;

&lt;p&gt;Using this approximation for (\ref{3}), we derive the following:&lt;/p&gt;

&lt;p&gt;\[1&amp;lt;\sum_{k=m}^{n}\dfrac{1}{k-1}=\sum_{k=m-1}^{n-1}\dfrac{1}{k}=\sum_{k=1}^{n-1}\dfrac{1}{k}-\sum_{k=1}^{m-2}\dfrac{1}{k}\approx\ln(\dfrac{n-1}{m-2})\]&lt;/p&gt;

&lt;p&gt;Similarly, for (\ref{4}), we obtain:&lt;/p&gt;

&lt;p&gt;\[1&amp;gt;\sum_{k=m+1}^{n}\dfrac{1}{k-1}=\sum_{k=m}^{n-1}\dfrac{1}{k}=\sum_{k=1}^{n-1}\dfrac{1}{k}-\sum_{k=1}^{m-1}\dfrac{1}{k}\approx\ln(\dfrac{n-1}{m-1})\]&lt;/p&gt;

&lt;p&gt;Alright, almost done! Notice that we can make another approximation of these results for large $ n $.&lt;/p&gt;

&lt;p&gt;For sufficiently large $ n $, the constants in our expressions will make negligible difference in our final result, allowing us to make the following claim:&lt;/p&gt;

&lt;p&gt;\[1\approx\ln(\dfrac{n}{m})\]&lt;/p&gt;

&lt;p&gt;Now, using the properties of natural logarithm, value of $e$ and then taking it’s inverse, we get:&lt;/p&gt;

&lt;p&gt;\[\boxed{(\dfrac{m}{n})\approx(\dfrac{1}{e})\approx 0.37}\]&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Q.E.D.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This result was proven through a lot of (good) approximations, but other more rigorous methods including those used in optimization come to the same result.&lt;/p&gt;

&lt;p&gt;So here we have it! A proof of why you should keep $37\%$ on your mind when it comes to matters of making choices.&lt;/p&gt;
</description>
				<pubDate>Sat, 24 Aug 2019 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/post/optimal-stopping-problem/</link>
				<guid isPermaLink="true">http://localhost:4000/post/optimal-stopping-problem/</guid>
			</item>
		
			<item>
				<title>Inference in probabilistic models</title>
				<description>&lt;p&gt;Statistical Machine Learning algorithms try to learn the structure of data by fitting a parametric distribution $p(x;θ)$ to it. Given a dataset, if we can represent it with a distribution, we can:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Generate new data “for free” by sampling from the learned distribution in silico; no need to run the true generative process for the data. This is a useful tool if the data is expensive to generate, i.e. a real-world experiment that takes a long time to run. Sampling is also used to construct estimators of high-dimensional integrals over spaces.&lt;/li&gt;
  &lt;li&gt;Evaluate the likelihood of data observed at test time (this can be used for rejection sampling or to score how good our model is).&lt;/li&gt;
  &lt;li&gt;Find the conditional relationship between variables. For example, learning the distribution $p(x_2\vert x_1)$ allows us to build discriminative classification or regression models.&lt;/li&gt;
  &lt;li&gt;Score our algorithm by using complexity measures like entropy, mutual information, and moments of the distribution.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;: $x\sim p_{X}$, $z=f(x)$.&lt;br /&gt;
&lt;strong&gt;Generation&lt;/strong&gt;: $z\sim p_{Z}$, $x=f^{-1}(z)$&lt;/p&gt;

&lt;p&gt;How can we use a model $p(\mathbf{x}, \mathbf{z})$ to analyze some data $\mathbf{x}$? In other words, what hidden structure of $\mathbf{z}$ explains the data? We seek to infer this hidden structure using the model.&lt;/p&gt;

&lt;p&gt;One method of inference leverages Bayes’ rule to define the posterior
\[p(\mathbf{z} \mid \mathbf{x}) = \frac{p(\mathbf{x}, \mathbf{z})}{\int_{z} p(\mathbf{x}, \mathbf{z}) \text{d}\mathbf{z}}.\]
​​ 
The posterior is the distribution of the latent variables $\mathbf{z}$, conditioned on some (observed) data $\mathbf{x}$. Drawing analogy to representation learning, it is a probabilistic description of the data’s hidden representation. Often this distribution is unknown and difficult to compute due to intractable denominator (which has to be computed over all comfiguration of states). Hence, we go for approximations.&lt;/p&gt;

&lt;p&gt;$2$ methods exist: Sampling methods a.k.a. Monte-Carlo methods and Variational inference.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sampling methods&lt;/strong&gt; involves drawing random samples from an unknown true distribution. Some methods are Importance sampling, Rejection sampling, Metropolis-hastings method and Gibbs sampling. The last two mthods falls under category of MCMC methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variational methods&lt;/strong&gt; involves approximating the unknown true distribution $p(\mathbf{x})$ with a simpler known distribution $q(\mathbf{x};\theta)$ parametrized by $\theta$. Basically we minimize the divergence between these distributions over $\theta$. To approximate $p(\mathbf{x})$, &lt;em&gt;variational lower bound&lt;/em&gt; is the key. We then maximize this VB to approximate $q$ to $p$. Its connections with deeplearning are beautifully established in paper - &lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot;&gt;Auto-encoding Variational Bayes (Kingma &amp;amp; Welling, 2014)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;More on these methods could be read in the excellent book by David Mckay - `Information theory, Inference and Learning Algorithms’ and from blogs &lt;a href=&quot;https://medium.com/neuralspace/inference-in-probabilistic-models-monte-carlo-and-deterministic-methods-eae8800ee095&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html&quot;&gt;2&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;example-use-of-variational-inference-to-obtain-lower-bound-on-the-marginal-likelihood&quot;&gt;Example: Use of Variational Inference to obtain lower-bound on the marginal likelihood&lt;/h3&gt;
&lt;p&gt;Consider a general probabilistic model with observations $x$, latent variables $z$ over which we must marginalize, and model parameters $θ$. We introduce an approximate posterior distribution for the latent variables $q_{\phi}(z\vert x)$ and follow the variational principle (Jordan et al., 1999) to obtain a bound on the marginal likelihood:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} \log p_{θ}(x) &amp; = \log \int p_{θ}(x\vert z)p(z)dz \\ 
                              &amp; = \log \int \frac{q_{\phi}(z\vert x)}{q_{\phi}(z\vert x)} p_{θ}(x\vert z)p(z)dz \\
                              &amp; = \log \left( \mathbb{E}_{q} \left[ \frac{p( z)}{q_{\phi}(z\vert x)} . q_{\phi}(z\vert x) p_{θ}(x\vert z) \right] \right) \\
                              &amp; \geq \mathbb{E}_{q} \left[\log \left(\frac{p( z)}{q_{\phi}(z\vert x)} . q_{\phi}(z\vert x) p_{θ}(x\vert z)\right) \right]  \\
                              &amp; = \mathbb{E}_{q} \left[\log \left(\frac{p( z)}{q_{\phi}(z\vert x)}\right) + \log\left(q_{\phi}(z\vert x) p_{θ}(x\vert z)\right) \right]  \\
                              &amp; = -\mathbb{D}_{\mathrm{KL}} [q_{\phi}(z\vert x) \Vert p( z)] + \mathbb{E}_q [\log p(x\vert z)],
        \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where we have used Jensen’s inequality $ \left(f (\mathbb{E}[x]) \geq \mathbb{E} [f(x)]\right) $, when $f(\cdot)$ is concave, $p_θ (x\vert z)$ is a likelihood function and $p(z)$ is a prior over the latent variables. This bound is often referred to as the negative free energy $\mathcal{F}$ or as the evidence lower bound (ELBO). It consists of two terms: the first is the KL-divergence between the approximate posterior and the prior distribution (which acts as a regularizer), and the second is a reconstruction error. This bound provides a unified objective function for optimization of both the parameters $θ$ and $\phi$ of the model and variational approximation, respectively.&lt;/p&gt;

&lt;p&gt;Current best practice in variational inference performs this optimization using mini-batches and stochastic gradient descent, which is what allows variational inference to be scaled to problems with very large data sets. There are two problems that must be addressed to successfully use the variational approach:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;efficient computation of the derivatives of the expected log-likelihood &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\phi} \mathbb{E}_{q_{\phi}(z)}\left[\log p_{\theta} (x\vert z)\right]&lt;/script&gt;. This is tackled in the paper &lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot;&gt;Auto-encoding Variational Bayes (Kingma &amp;amp; Welling, 2014)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;choosing the richest, computationally-feasible approximate posterior distribution $q(·)$. This is tackled in the paper &lt;a href=&quot;/post/checksum/&quot;&gt;Variational Inference of Normalizing Flows (Rezende &amp;amp; Shakir, 2015)&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Thu, 08 Aug 2019 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/post/inf_prob_models/</link>
				<guid isPermaLink="true">http://localhost:4000/post/inf_prob_models/</guid>
			</item>
		
	</channel>
</rss>