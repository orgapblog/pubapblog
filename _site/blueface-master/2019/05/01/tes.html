<h1 id="manifold-learning">Manifold learning</h1>

<p>Manifold learning is basically using the geometric properties of data to exploit machine learning. It is frequently used in dimensionality reduction (KPCA, LLE, diffusion maps etc), clustering and semi-supervised/supervised learning. It has strong connections with differential geometry and assumes the data lies near a manifold embedded in high-dimensional space.</p>

<hr />

<h1 id="diffusion-maps">Diffusion Maps</h1>
<p>Diffusion maps approximate some differential operators on data manifold $\mathcal{M}$. (proofs in the literature show this in the $\lim N\rightarrow \infty, \epsilon \rightarrow 0$). Here, we take the Taylor expansions of the functions to arrive at differential operators.</p>

<hr />

<h1 id="why-is-laplacian-special">Why is Laplacian special??</h1>

<ul>
  <li>The usage of this operator comes up in various daily-life problems: heat equation, wave equation, fuild-flow, quantum mechanics etc &amp; more recently in manifold learning (thatâ€™s what this blog summarizes!).</li>
  <li>It is invariant to <em>translation  &amp; rotation</em>. In general if $S$ is an operator which satisfies such properties, then $ S=\sum_{j=1}^{m}a_{j}\Delta^{j} $.</li>
</ul>

<h3 id="solving-deltapsilambdapsi-">Solving $\Delta\psi=\lambda\psi$ !</h3>
<p>We want to solve $\Delta\psi=\lambda\psi$. A standard method (inspired by Stone-Weierstrass Theorem) is to look for solutions
$ u(x; t) $ of the form $u(x; t) = \alpha(t)\phi(x)$. If you do this into heat equation, we get</p>

<script type="math/tex; mode=display">\dfrac{\Delta\phi(x)}{\phi(x)}= - \dfrac{\Delta\alpha(x)}{\alpha(x)}</script>
