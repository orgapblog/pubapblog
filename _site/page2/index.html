<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="icon" type="image/icon" href="favicon.ico">


<title>Home</title>


<script src="https://storage.googleapis.com/montco-stats/javascript/vis-4.17.0/dist/vis.js" type="text/javascript"></script>
<link href="https://storage.googleapis.com/montco-stats/javascript/vis-4.17.0/dist/vis-network.min.css" rel="stylesheet" type="text/css" />

<link rel="stylesheet" href="http://localhost:4000/css/syntax.css">
<link rel="stylesheet" href="http://localhost:4000/css/main.css">
</head>
<body>


<header>
<div id="nav_container">
<nav>

  <ul>
  <h1><a href="">Research Notes</a></h1>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/about/">About</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/tutorials/">Tutorials</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/paper_summary/">Paper Summaries</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/distill/">Distills</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/tags/">Archives</a>
      </li>
    
  </ul>

</nav>
</div>
</header>

<section id="main">
	  
      <article>
      <div class="heading"><a href="/post/checksum/">Variational Inference of Normalizing Flows (Rezende & Shakir, 2015)</a></div>
      <p class="meta"><a class="permalink" href="/post/checksum/">&#9679;</a> 27 Aug 2019</p>
      <div>
        <p><a href="http://proceedings.mlr.press/v37/rezende15.pdf">Original paper link</a></p>

<h1 id="normalizing-flows">Normalizing Flows</h1>

<p><img src="\photos\shakir_danilo_slide.png" alt="" style="width: 600px; height: auto;" /></p>

<p><strong>Key idea</strong>: Assume a simple probability distribution, take a sample from it and then <strong><em>transform</em></strong> that sample. This is equivalent to change of variables in probability distributions and, if the transformation meets some mild conditions, can result in a very complex pdf of the transformed variable. The formalism of normalizing flows now gives us a systematic way of specifying the approximate posterior distributions $q(z\vert x)$ required for variational inference. (see <a href="/post/inf_prob_models/">Inference in probabilistic models</a>)</p>

<p>Example: If $f$ is monotonically increasing or monotonically decreasing function, such that $y = f(x)$, $x \sim p_{X}$, then we have:
\[ p_{Y}(y)= \frac{d}{dy}\mathbb{P}(Y\leq y)= \frac{d}{dy}\mathbb{P}(f(X)\leq y)=\frac{d}{dy}\mathbb{P}(X\leq f^{-1}(y))=p_{X}(f^{-1}(y))\frac{df^{-1}(y)}{dy}=p_{X}(x)\frac{df^{-1}(y)}{dy},\]
\[ p_{Y}(y)= \frac{d}{dy}\mathbb{P}(Y\leq y)= \frac{d}{dy}\mathbb{P}(f(X)\leq y)=\frac{d}{dy}\mathbb{P}(X\geq f^{-1}(y))=-p_{X}(f^{-1}(y))\frac{df^{-1}(y)}{dy}=-p_{X}(x)\frac{df^{-1}(y)}{dy},\]
respectively.</p>

<p>Similarly for multivariate invertible mappings $f:\mathbb{R}^{m}\mapsto \mathbb{R}^{m}$ and $X\sim p_{X}$, $Y=f(X)$, we get:</p>

<p>\[\boxed{ p_{Y}(y)=p_{X}(x)\left\lvert \operatorname{det}\frac{df^{-1}(y)}{dy}\right\rvert =p_{X}(x)\left\lvert \operatorname{det}\frac{df(x)}{dy}\right\rvert^{-1} },\qquad complexity \rightarrow O(m^{3})\]
where the last equality exploits the <a href="https://en.wikipedia.org/wiki/Inverse_function_theorem">property of jacobian of inverse functions</a> (PS: invertibility of $f(\cdot)$ validates the use of this property) and $[ \mathrm{det}(A^{-1}) =\mathrm{det}(A)^{-1} ]$.</p>

<p>NFs are typically used to parametrise the approximate posterior $q(z\vert x)$ but can also be applied for the likelihood function. We can apply a series of mappings  $f_{k},~ k= [ 1,\dots,K ]$ and obtain a normalizing flow:</p>

<script type="math/tex; mode=display">\mathbf{z}_K = f_K \circ \dots \circ f_1 (\mathbf{z}_0), \quad \mathbf{z}_0 \sim q_0(\mathbf{z}_0),</script>

<script type="math/tex; mode=display">\mathbf{z}_K \sim q_K(\mathbf{z}_K) = q_0(\mathbf{z}_0) \prod_{k=1}^K
  \left|
    \mathrm{det} \frac{
      \partial f_k
    }{
      \partial \mathbf{z}_{k-1}\
    }
  \right| ^{-1}, \quad or</script>

<script type="math/tex; mode=display">\ln q_K (\mathbf{z}_K) = \ln q_0(\mathbf{z}_0) - \sum_{k=1}^{K} \ln \mathrm{det} \left|
     \frac{
      \partial f_k
    }{
      \partial \mathbf{z}_{k-1}\
    }
  \right|.</script>

<p>This series of transformations can transform a simple probability distribution (e.g. Gaussian) into a complicated multi-modal one. Note that after every mapping ($f_1, f_2, \dots $), the det-Jac has to be computed to ensure that the output is also a distribution. To be of practical use, however, we can consider only transformations whose determinants of Jacobians are easy to compute. The original paper considered two simple family of transformations, named planar and radial flows.</p>

<hr />

<h2 id="planar-flow">Planar Flow</h2>
<p>\[ f(\mathbf{z}) = \mathbf{z} + \mathbf{u} h(\mathbf{w}^{\top} \mathbf{z} + b) \]
with $\mathbf{U}, \mathbf{W}\in \mathbb{R}^{d}$ and $b\in \mathbb{R}$ and $h$ an element-wise non-linearity. Let $\psi (\mathbf{z}) = h’ (\mathbf{w}^T \mathbf{z} + b) \mathbf{w}$. Then the determinant can be easily computed as</p>

<p>\[ \left| \mathrm{det} \frac{\partial f}{\partial \mathbf{z}} \right| =
  \left| 1 + \mathbf{u}^{\top} \psi( \mathbf{z} ) \right| = \left| 1 + \mathbf{u}^{\top} h’ (\mathbf{w}^T \mathbf{z} + b) \mathbf{w} \right| .  \]
Since $0\leq h’ \leq 1$ for $ h(x) = \tanh (x) $, we require $ \mathbf{u}^{\top}\mathbf{w} \geq -1$ for invertibility of such flows. We can think of planar flows as slicing the $\mathbf{z}$-space with straight lines (or hyperplanes), where each line contracts or expands the space around it, see figure 1 (in paper).</p>

<hr />

<h2 id="radial-flow">Radial Flow</h2>
<p>\[  f(\mathbf{z}) = \mathbf{z} + \beta h(\alpha, r)(\mathbf{z} - \mathbf{z}_0),\]</p>

<p>with <script type="math/tex">r = \Vert\mathbf{z} - \mathbf{z}_0 \Vert_2</script>, <script type="math/tex">h(\alpha, r) = \frac{1}{\alpha + r}</script> and parameters <script type="math/tex">\mathbf{z}_0 \in \mathbb{R}^d, \alpha \in \mathbb{R}_+</script> and <script type="math/tex">\beta \in \mathbb{R}</script>.</p>

<p>Similarly to planar flows, radial flows introduce spheres in the $z$-space, which either contract or expand the space inside the sphere, see figure 1 (in paper).</p>

<hr />
<h3 id="algorithmic-steps">Algorithmic steps</h3>
<p>Suppose the unknown target distribution is specified using energy functions $U(z)$, $p(z) = \frac{1}{\mathcal{Z}} \exp(-U(z))$, where $\mathcal{Z}$ is the unknown partition function (normalization constant); that is, $p(z) \propto \exp({-U(z)})$.</p>

<p><strong>Steps:</strong></p>
<ol>
  <li>
    <p>Generate random samples from initial distribution $z_{0} \sim q_0 (z) = \mathcal{N}(z; \mu, \sigma^2 I)$.
Here $\mu$ and $\sigma$ can either be fixed (such as standard Normal distribution) or can be estimated as follows:
Draw auxillary random variable $\epsilon$ from standard normal distribution $\epsilon \sim \mathcal{N}(0, I)$ and apply linear normalizing flow transformation $f(\epsilon) = \mu + \sigma \epsilon$, re-parameterizing $\sigma = e^{(\frac{1}{2}*\log(var))}$ to ensure $\sigma &gt; 0$, then jointly optimize {$\mu$, $var$} together with the other normalizing flow parameters (see below).</p>
  </li>
  <li>Transform the initial samples $z_0$ through $K$ <em>Normalizing Flow</em> transforms, from which we obtain the transformed approximate distribution $q_{K}(z)$,
  $\log q_{K}(z) = \log q_0 (z) - \sum_{k=1}^K \log (\mathrm{det} |J_k|)$
where $J_k$ is the Jacobian of the $k$-th (invertible) normalizing flow transform.
E.g. for planar flows,
  $\log q_{K}(z) = \log q_0 (z) - \sum_{k=1}^K \log |1 + u_k^{\top} \psi_k(z_{k-1})|$
where each flow includes model parameters $\lambda = \{ w, u, b \}$.</li>
  <li>Jointly optimize all model parameters by minimizing <em>KL-Divergence</em> between the approximate distribution $q_{K}(z)$
and the true distribution $p(z)$.
  <script type="math/tex">% <![CDATA[
\begin{align*}
           loss = KL[q_{K}(z)||p(z)] & = \mathbb{E}_{z_K \sim q_{K}(z)} \left[ \log q_K(z_K) - \log p(z_K) \right]\\
                          &= \mathbb{E}_{z_K \sim q_{K}(z)} \left[ \left(\log q_0(z_0) - \sum_k \log (\mathrm{det} |J_k|)\right) - \left(-U(z_K) - \log(\mathcal{Z})\right) \right]\\
                         &= \mathbb{E}_{z_0 \sim q_0(z)} \left[ \log q_0(z_0) - \sum_k \log \left(\mathrm{det} |J_k|\right) + U(f_1(f_2(\dots f_K(z_0)))) + \log(\mathcal{Z}) \right]
  \end{align*} %]]></script><br />
Here the partition function $\mathcal{Z}$ is independent of $z_0$ and model parameters, so we can ignore it for the optimization
  $loss = \mathbb{E}_{z_0 \sim q_0(z)} \left[ \log q_0(z_0) - \sum_k \log (\mathrm{det} |J_k|) + U(z_K) \right]$</li>
</ol>

<p>The expectation could be approximated by Monte Carlo sampling, the mini-batch average here could be considered as the Monte Carlo Sampling Expectation.</p>

<h3 id="ex-density-estimation-generative-modelling">Ex: Density estimation (Generative modelling)</h3>
<p>$p_X:$ data distribution, $p_Y:$ prior over latent variable, $p_{f^{-1}(Y)}:$ push-forward density of $f^{-1}(Y)$, i.e the generative model</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*} 
 D_{KL}(p_X \Vert p_{f^{-1}(Y)}) & = \mathbb{E}_{x\sim p_{X}}\left[ \log p_X (x) - \log p_{f^{-1}(Y)} (x) \right] \\ 
                                 & = \mathbb{E}_{x\sim p_{X}}\left[ \log p_X (x) - \log p_{X} (x) \right] \\
                                 & = \mathbb{E}_{x\sim p_{X}}\left[ \log p_X (x) - \log p_{Y} (f(x))\left| \mathrm{det} \frac{\partial f(x)}{\partial x} \right|\right]\\
                                 & = \mathbb{E}_{x\sim p_{X}}\left[ \log p_X (x) - \log p_{Y} (f(x)) - \log \left| \mathrm{det} \frac{\partial f(x)}{\partial x} \right| \right]
\end{align*} %]]></script>

<hr />

<h3 id="footnotes">Footnotes:</h3>
<ol>
  <li>
    <p>See <a href="https://arxiv.org/abs/1511.01844">A note on the evaluation of generative models</a> for a thought-provoking discussion about how high log-likelihood is neither sufficient nor necessary to generate “plausible” images. Still, it’s better than nothing and in practice a useful diagnostic tool.</p>
  </li>
  <li>
    <p>There’s a connection between Normalizing Flows and GANs via encoder-decoder GAN architectures that learn the inverse of the generator (ALI / BiGAN). Since there is a separate encoder trying to recover $u=G^{−1}(x)$ such that $x=G(u)$, the generator can be thought of as a flow for the simple uniform distribution. However, we don’t know how to compute the amount of volume expansion/contraction w.r.t. $x$, so we cannot recover density from GANs. However, it’s probably not entirely unreasonable to model the log-det-jacobian numerically or enforce some kind of linear-time Jacobian by construction.</p>
  </li>
</ol>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#paper_summary-ref" title="View posts tagged with &quot;paper_summary&quot;">paper_summary</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/post/optimal-stopping-problem/">Optimal Stopping Problem</a></div>
      <p class="meta"><a class="permalink" href="/post/optimal-stopping-problem/">&#9679;</a> 24 Aug 2019</p>
      <div>
        <p>There are a lot of everyday situations where people make decisions one after the other, and what is decided earlier affects the choices later on. Most people probably go by some gut feeling about when it’s time to stop and select the next best option and settle down.</p>

<p>In this article we will examine the “Optimal Stopping Problem,” a mathematical puzzle that can manifest in many different real world examples. For instance:</p>

<ul>
  <li>When should a hiring manager stop interviewing people for a new job position? <br />
This is the classical “secretary problem” within the field of study of mathematics: probability-optimization. This problem in it’s simplest form has following features:
    <ul>
      <li>There is one secretarial position available.</li>
      <li>The number of applicants is $n$.</li>
      <li>The applicants are interviewed sequentially in random order, each order being equally likely.</li>
      <li>After each interview the hiring manager must decide to reject or hire the applicant.</li>
      <li>An applicant once rejected cannot later be recalled.</li>
      <li>Once the hiring manager decides to hire an applicant his/her search is over and there is no need to interview any more candidates.<br />
This basic problem has a remarkably simple solution and was outlined in the basic paper by  Gilbert and Mosteller ( 1966 ), with elegant derivations and extensions in a number of important directions.</li>
    </ul>
  </li>
  <li>
    <p>We have a house and we wish to sell it. Each day we get a new offer for the house, which could be higher or lower than the previous offer; however, each day the house is on the market we must pay for advertising. We wish to maximise the amount you earn by choosing a stopping rule. When should we stop looking at potential buyers and finally settle the deal?</p>
  </li>
  <li>
    <p>At some point in a lot of peoples lives, we wonder when we should stop dating and finally settle down with the right partner. Most people probably go by some gut feeling about when it’s time to stop. Editor’s Note: Of course, for mathematicians this is a purely theoretical example.</p>
  </li>
  <li>Suppose we are a doctor and patients arrive sequentially at our clinic and must be treated immediately by one of the treatments. It is assumed that response from treatment is immediate so that the effectiveness of the treatment that the present patient receives is known before the next patient is treated. It is not known precisely which one of the treatments is best, but we must decide which treatment to give each patient, keeping in mind that our goal is to cure as many patients as possible. This may require us to give a patient a treatment which is not the one that looks best at the present time in order to gain information that may be of use to future patients. When should we <strong>stop evaluating</strong> more treatments and finally select the best one?</li>
</ul>

<p>There are many different solutions to the problems listed above, as the strategy we choose will depend on our objective. Do we want to ensure that the option we select is better than average? What if we just want to be sure that we do not pick the worst option? In this article our goal will be to “<strong>minimize the risk</strong>” of missing the best option. In other words, we want to “maximize the chances” (i.e. probability) of selecting the best option.</p>

<h3 id="the-prelude">The Prelude</h3>

<p>The general form of our strategy will be to “gather information” when we are presented with our first options. We know that we will not choose any of these; however, we will examine them in order to know “how good” we should expect our options to be. After a certain point, once we are done gathering information, we will choose the first option that was better than all the ones we have seen before. After some thought, we can see that this strategy makes some intuitive sense. We don’t want to choose an option too early since there would be a high likelihood a better option would come later. But when should we stop gathering information? While we don’t want to choose an option too early, we certainly don’t want to gather information for too long and risk the best option passing us by.</p>

<p>This type of problem is called the “<strong>Optimal Stopping Problem</strong>”, which mathematicians and computer scientists have researched for many years.</p>

<h3 id="a-real-life-example">A Real-life Example</h3>

<p>Suppose we want to sell our house. We have a pool of n potential buyers (we number them from $1$ to $n$), who we meet one after the other. After a potential buyer has made their offer, we must decline or accept their offer. If we decide to accept their offer, we can no longer sell the house to anyone else and if we reject an offer the buyer will take their business elsewhere, and we will not be able to change our minds later on. For simplicity’s sake, we will assume there is no advertising cost (i.e. we do not have to pay to keep our house on the market).</p>

<p>Among our pool of $n$ people, there’s at least one who gives us the highest value for our property. We will call that person  $m$  (where  $m$  ranges from  $1$  to  $n$) – it’s who we’d ideally want to end up making deal with.</p>

<p>Our strategy is to discuss the deal with  $m-1$  of the $n$ people and then settle with the next best person who is better. (The reason for doing the calculations with  $m-1$  is that  m is the minimum number of potential buyers that we will encounter; there are  $m-1$  who give us the data we need before we start searching, and the $m^{th}$ is the first one who can be accepted.)</p>

<p>Given a pool of $n$ potential buyers, let $P(m; n)$ represent the probability that we will choose the best offer if we use the strategy discussed earlier. I.e. If there are $n$ buyers and we “gather information” during the first $m-1$ offers and pick the first offer that is better than the current best, the probability that we will select the best possible offer is $P(m; n)$.</p>

<p>Notice immediately that $P(1; n) = \dfrac{1}{n}$ and $P(n; n) = \dfrac{1}{n}$. These two cases essentially give no choice, meaning, it is just like the probability of picking one person at random out of total number of people ($n$) and hence the probability is $\dfrac{1}{n}$. But interesting things happen ‘<strong><em>in between</em></strong>’ these two extremes. As we see more people , we already have a notion of the “best so far,” and the next person we see that beats the current “best so far” is more likely to be the best person in the group for us. Unfortunately, it works the other way around as well. As we continue to see more people, the likelihood of having already passed the best so far increases, thereby decreasing our chances at finding the optimal person.</p>

<p>We want to find the optimal place to stop – a sort of middle ground that maximizes our chances of choosing the optimal person.</p>

<p>To do this, we want to maximize $P(m; n)$. In the process of doing this, we want to make sure our solution generalizes. A good way to generalize is to find the right ratio $\dfrac{m}{n}$, the percentage of our population that we should analyze before making a decision.</p>

<h4 id="mathematical-formalism">Mathematical Formalism</h4>
<p>Our first step here is to get an equation for $P(m; n)$.</p>

<p>Let’s say we have collected information about $m - 1$ potential buyers and now are looking at the $k^{th}$ person in the sequence, where  $m-1 \leq k \leq n$.</p>

<p>Now let’s break down some things that are going on here:</p>
<ul>
  <li>We know that this  $k^{th}$  person has a $\dfrac{1}{n}$ probability of being the best buyer.</li>
  <li>We also know that in order to be considering the $k^{th}$ buyer in this sequence, the highest ranking person thus far must be in the group of $m - 1$ people out of the $k - 1$ people we had previously inspected and rejected (otherwise we would have stopped at someone before the $k^{th}$ person).</li>
</ul>

<p>And the probability of the highest ranked person thus far being in that group of $m - 1$ people out of the $k - 1$ people inspected is:
\[ \dfrac{m - 1}{k - 1}. \]</p>

<p>And therefore, the overall probability of finding the best potential partner this time is:
\[ (\dfrac{m - 1}{k - 1}).(\dfrac{1}{n}) = \dfrac{m - 1}{n(k - 1)}\]</p>

<p>Note: In the following section we will use summation notation ($ \sum $ symbol), which works as follows: Suppose we have a sequence of numbers $x_1, x_2, …, x_n$, then, by definition, $\sum_{i=1}^{n} x_i = x_1 + x_2 + x_3 + … + x_n$. Thus $\sum_{i=1}^{n} 1 = n$ and $\sum_{i=1}^{3} x_i = x_1 + x_2 + x_3$. We use this notation because it is shorter than writing out each term in the sum.</p>

<p>Now to determine $ P(m; n) $, we must remember that $ k $ can take on any value from $ m $ to $ n $. As a consequence, we must sum these probabilities for any value of $ k $ (note the range of $ k $ above). A compact way to express this is as follows:
\[P(m; n) =\sum_{k=m}^{n} \dfrac{m-1}{n(k - 1)}= \dfrac{m-1}{n}\sum_{k=m}^{n}\dfrac{1}{k-1}. \tag{1} \label{1}\]</p>

<p>At this point, it is possible to determine the maximum value using methods of optimization one learns in calculus, but this can get nasty pretty quickly. We will lay out a more elegant (albeit more mathematically involved) approach in the following section.</p>

<h4 id="to-get-more-technical">To Get More Technical</h4>
<p>Given a number $n$, we want to maximize the value of $P(m;n)$. Hence, the optimal value for $ m $ will be such that:
       \[P(m - 1; n) &lt; P(m; n) \text{ and } P(m + 1; n) &lt; P(m;n) \tag{2} \label{2}\]</p>

<p>Author’s Note: If you want to be very pedantic, you could ask what happens if there are two “best” values of $ m $, with one of those strict inequality signs replaced by a partial inequality. It turns out that the only time when equality is possible is when $ n=2 $, which is not very interesting anyway.</p>

<p>Let’s break this down part by part. From the first inequality in (\ref{2}) and using (\ref{1}):
\[P(m - 1; n) &lt; P (m; n)\]</p>

<p>\[\dfrac{m-2}{n}\sum_{k=m-1}^{n} \dfrac{1}{k - 1}&lt;\dfrac{m-1}{n} \sum_{k=m}^{n}\dfrac{1}{k-1}\]</p>

<p>\[(m-2)\sum_{k=m-1}^{n} \dfrac{1}{k - 1}&lt;(m-1) \sum_{k=m}^{n}\dfrac{1}{k-1}.\]</p>

<p>Rewriting the left hand side:</p>

<p>\[(m-2)(\dfrac{1}{m-2})+\sum_{k=m}^{n} \dfrac{1}{k - 1}&lt;(m-1) \sum_{k=m}^{n}\dfrac{1}{k-1}.\]</p>

<p>After algebraic simplification, we get:
    \[1&lt;\sum_{k=m}^{n}\dfrac{1}{k-1}\tag{3} \label{3}\]</p>

<p>Let’s now do the same thing for the other inequality from (\ref{2}):</p>

<p>\[P (m; n) &gt; P (m + 1; n)\]</p>

<p>\[(m-1)\sum_{k=m}^{n}\dfrac{1}{k-1}&gt;m\sum_{k=m+1}^{n}\dfrac{1}{k-1}\]</p>

<p>Performing a similar calculations to what we did with the last inequality, we get:</p>

<p>\[(m-1)(\dfrac{1}{m-1}+\sum_{k=m+1}^{n}\dfrac{1}{k-1})&gt;m\sum_{k=m+1}^{n}\dfrac{1}{k-1}\]</p>

<p>\[(1+(m-1)\sum_{k=m+1}^{n}\dfrac{1}{k-1})&gt;m\sum_{k=m+1}^{n}\dfrac{1}{k-1}\]</p>

<p>\[1&gt;\sum_{k=m+1}^{n}\dfrac{1}{k-1}\tag{4}\label{4}\]</p>

<p>Let’s remember that our goal here is to find an appropriate approximation for the ratio $ \dfrac{m}{n} $ for the best $ m  $.</p>

<p>One fact that is going to help us immensely in this calculation is the approximation of the partial sum of a harmonic series. Mathematically, the approximation is:</p>

<p>\[\sum_{k=1}^{x}\dfrac{1}{k}\approx\ln(x)+c \]</p>

<p>where $ c $ is the Euler-Mascheroni constant.</p>

<p>Now this approximation greatly helps us simplify inequalities (\ref{3}) and (\ref{4}).</p>

<p>Using this approximation for (\ref{3}), we derive the following:</p>

<p>\[1&lt;\sum_{k=m}^{n}\dfrac{1}{k-1}=\sum_{k=m-1}^{n-1}\dfrac{1}{k}=\sum_{k=1}^{n-1}\dfrac{1}{k}-\sum_{k=1}^{m-2}\dfrac{1}{k}\approx\ln(\dfrac{n-1}{m-2})\]</p>

<p>Similarly, for (\ref{4}), we obtain:</p>

<p>\[1&gt;\sum_{k=m+1}^{n}\dfrac{1}{k-1}=\sum_{k=m}^{n-1}\dfrac{1}{k}=\sum_{k=1}^{n-1}\dfrac{1}{k}-\sum_{k=1}^{m-1}\dfrac{1}{k}\approx\ln(\dfrac{n-1}{m-1})\]</p>

<p>Alright, almost done! Notice that we can make another approximation of these results for large $ n $.</p>

<p>For sufficiently large $ n $, the constants in our expressions will make negligible difference in our final result, allowing us to make the following claim:</p>

<p>\[1\approx\ln(\dfrac{n}{m})\]</p>

<p>Now, using the properties of natural logarithm, value of $e$ and then taking it’s inverse, we get:</p>

<p>\[\boxed{(\dfrac{m}{n})\approx(\dfrac{1}{e})\approx 0.37}\]</p>

<p><em>Q.E.D.</em></p>

<p>This result was proven through a lot of (good) approximations, but other more rigorous methods including those used in optimization come to the same result.</p>

<p>So here we have it! A proof of why you should keep $37\%$ on your mind when it comes to matters of making choices.</p>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/post/inf_prob_models/">Inference in probabilistic models</a></div>
      <p class="meta"><a class="permalink" href="/post/inf_prob_models/">&#9679;</a> 08 Aug 2019</p>
      <div>
        <p>Statistical Machine Learning algorithms try to learn the structure of data by fitting a parametric distribution $p(x;θ)$ to it. Given a dataset, if we can represent it with a distribution, we can:</p>

<ol>
  <li>Generate new data “for free” by sampling from the learned distribution in silico; no need to run the true generative process for the data. This is a useful tool if the data is expensive to generate, i.e. a real-world experiment that takes a long time to run. Sampling is also used to construct estimators of high-dimensional integrals over spaces.</li>
  <li>Evaluate the likelihood of data observed at test time (this can be used for rejection sampling or to score how good our model is).</li>
  <li>Find the conditional relationship between variables. For example, learning the distribution $p(x_2\vert x_1)$ allows us to build discriminative classification or regression models.</li>
  <li>Score our algorithm by using complexity measures like entropy, mutual information, and moments of the distribution.</li>
</ol>

<p><strong>Inference</strong>: $x\sim p_{X}$, $z=f(x)$.<br />
<strong>Generation</strong>: $z\sim p_{Z}$, $x=f^{-1}(z)$</p>

<p>How can we use a model $p(\mathbf{x}, \mathbf{z})$ to analyze some data $\mathbf{x}$? In other words, what hidden structure of $\mathbf{z}$ explains the data? We seek to infer this hidden structure using the model.</p>

<p>One method of inference leverages Bayes’ rule to define the posterior
\[p(\mathbf{z} \mid \mathbf{x}) = \frac{p(\mathbf{x}, \mathbf{z})}{\int_{z} p(\mathbf{x}, \mathbf{z}) \text{d}\mathbf{z}}.\]
​​ 
The posterior is the distribution of the latent variables $\mathbf{z}$, conditioned on some (observed) data $\mathbf{x}$. Drawing analogy to representation learning, it is a probabilistic description of the data’s hidden representation. Often this distribution is unknown and difficult to compute due to intractable denominator (which has to be computed over all comfiguration of states). Hence, we go for approximations.</p>

<p>$2$ methods exist: Sampling methods a.k.a. Monte-Carlo methods and Variational inference.</p>

<p><strong>Sampling methods</strong> involves drawing random samples from an unknown true distribution. Some methods are Importance sampling, Rejection sampling, Metropolis-hastings method and Gibbs sampling. The last two mthods falls under category of MCMC methods.</p>

<p><strong>Variational methods</strong> involves approximating the unknown true distribution $p(\mathbf{x})$ with a simpler known distribution $q(\mathbf{x};\theta)$ parametrized by $\theta$. Basically we minimize the divergence between these distributions over $\theta$. To approximate $p(\mathbf{x})$, <em>variational lower bound</em> is the key. We then maximize this VB to approximate $q$ to $p$. Its connections with deeplearning are beautifully established in paper - <a href="https://arxiv.org/abs/1312.6114">Auto-encoding Variational Bayes (Kingma &amp; Welling, 2014)</a>.</p>

<p>More on these methods could be read in the excellent book by David Mckay - `Information theory, Inference and Learning Algorithms’ and from blogs <a href="https://medium.com/neuralspace/inference-in-probabilistic-models-monte-carlo-and-deterministic-methods-eae8800ee095">1</a>, <a href="http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html">2</a>.</p>

<hr />

<h3 id="example-use-of-variational-inference-to-obtain-lower-bound-on-the-marginal-likelihood">Example: Use of Variational Inference to obtain lower-bound on the marginal likelihood</h3>
<p>Consider a general probabilistic model with observations $x$, latent variables $z$ over which we must integrate, and model parameters $θ$. We introduce an approximate posterior distribution for the latent variables $q_{\phi}(z\vert x)$ and follow the variational principle (Jordan et al., 1999) to obtain a bound on the marginal likelihood:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*} \log p_{θ}(x) & = \log \int p_{θ}(x\vert z)p(z)dz \\ 
                              & = \log \int \frac{q_{\phi}(z\vert x)}{q_{\phi}(z\vert x)} p_{θ}(x\vert z)p(z)dz \\
                              & = \log \left( \mathbb{E}_{q} \left[ \frac{p( z)}{q_{\phi}(z\vert x)} . q_{\phi}(z\vert x) p_{θ}(x\vert z) \right] \right) \\
                              & \geq \mathbb{E}_{q} \left[\log \left(\frac{p( z)}{q_{\phi}(z\vert x)} . q_{\phi}(z\vert x) p_{θ}(x\vert z)\right) \right]  \\
                              & \geq \mathbb{E}_{q} \left[\log \left(\frac{p( z)}{q_{\phi}(z\vert x)}\right) + \log\left(q_{\phi}(z\vert x) p_{θ}(x\vert z)\right) \right]  \\
                              & = -\mathbb{D}_{\mathrm{KL}} [q_{\phi}(z\vert x) \Vert p( z)] + \mathbb{E}_q [\log p(x\vert z)],
        \end{align*} %]]></script>

<p>where we have used Jensen’s inequality $ \left(f (\mathbb{E}[x]) ≤ \mathbb{E} [f(x)]\right) $. $p_θ (x\vert z)$ is a likelihood function and $p(z)$ is a prior over the latent variables. This bound is often referred to as the negative free energy $\mathcal{F}$ or as the evidence lower bound (ELBO). It consists of two terms: the first is the KL-divergence between the approximate posterior and the prior distribution (which acts as a regularizer), and the second is a reconstruction error. This bound provides a unified objective function for optimization of both the parameters $θ$ and $\phi$ of the model and variational approximation, respectively.</p>

<p>Current best practice in variational inference performs this optimization using mini-batches and stochastic gradient descent, which is what allows variational inference to be scaled to problems with very large data sets. There are two problems that must be addressed to successfully use the variational approach:</p>

<ol>
  <li>efficient computation of the derivatives of the expected log-likelihood <script type="math/tex">\nabla_{\phi} \mathbb{E}_{q_{\phi}(z)}\left[\log p_{\theta} (x\vert z)\right]</script>. This is tackled in the paper <a href="https://arxiv.org/abs/1312.6114">Auto-encoding Variational Bayes (Kingma &amp; Welling, 2014)</a></li>
  <li>choosing the richest, computationally-feasible approximate posterior distribution $q(·)$. This is tackled in the paper <a href="/post/checksum/">Variational Inference of Normalizing Flows (Rezende &amp; Shakir, 2015)</a>.</li>
</ol>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/post/latexsublime-workflow/">Latex+Sublime+MathJax+Jekyll workflow</a></div>
      <p class="meta"><a class="permalink" href="/post/latexsublime-workflow/">&#9679;</a> 11 May 2019</p>
      <div>
        <p>So basically to make heavy math websites faster, use the workflow as below:</p>

<ol>
  <li>Have the pre-requisites installed first i.e. latex using miktex and Sublime text 3.</li>
  <li>Make sure you have Perl also on your system.</li>
  <li>Add “latexindent.pl” package from miktex.</li>
  <li>Now set-up your sublime text as follows:
    <ul>
      <li>First things first: install ‘package control’</li>
      <li>Then add the following packages: LaTexTools, LateXYZ, Jekyll, BeautifyLatex</li>
      <li>Optionally add these packages just to make your experience better: materialtheme, side bar, advanced new file, file icon and GitGutter.</li>
    </ul>
  </li>
  <li>If needed, use Pandoc to convert tex file to html</li>
</ol>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">pandoc</span> <span class="o">-</span><span class="n">s</span> <span class="n">name_of_tex_file</span><span class="o">.</span><span class="n">tex</span> <span class="o">--</span><span class="n">mathjax</span> <span class="o">-</span><span class="n">o</span> <span class="n">name_of_ex_file</span><span class="o">.</span><span class="n">html</span> </code></pre></figure>

<p>That’s it. Your set-up is done!</p>

<p>To work faster with Sublime, learning keybindings (a.k.a keyboard shortcuts) is a must.</p>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  


<ul class="pager">
  
      <li class="previous"><a href="/">&larr; Newer</a></li>
  
    
      <li class="next"><a href="/page3">Older &rarr;</a></li>
  
</ul>
</section>





</body>
</html>
