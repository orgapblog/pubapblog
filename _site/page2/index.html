<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="icon" type="image/icon" href="favicon.ico">


<title>Home</title>


<script src="https://storage.googleapis.com/montco-stats/javascript/vis-4.17.0/dist/vis.js" type="text/javascript"></script>
<link href="https://storage.googleapis.com/montco-stats/javascript/vis-4.17.0/dist/vis-network.min.css" rel="stylesheet" type="text/css" />

<link rel="stylesheet" href="http://localhost:4000/css/syntax.css">
<link rel="stylesheet" href="http://localhost:4000/css/main.css">
</head>
<body>


<header>
<div id="nav_container">
<nav>

  <ul>
  <h1><a href="">Research Notes</a></h1>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/about/">About</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/tutorials/">Tutorials</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/paper_summary/">Paper Summaries</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/distill/">Distills</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/tags/">Archives</a>
      </li>
    
  </ul>

</nav>
</div>
</header>

<section id="main">
	  
      <article>
      <div class="heading"><a href="/post/inf_prob_models/">Inference in probabilistic models</a></div>
      <p class="meta"><a class="permalink" href="/post/inf_prob_models/">&#9679;</a> 08 Aug 2019</p>
      <div>
        <p>Statistical Machine Learning algorithms try to learn the structure of data by fitting a parametric distribution $p(x;θ)$ to it. Given a dataset, if we can represent it with a distribution, we can:</p>

<ol>
  <li>Generate new data “for free” by sampling from the learned distribution in silico; no need to run the true generative process for the data. This is a useful tool if the data is expensive to generate, i.e. a real-world experiment that takes a long time to run. Sampling is also used to construct estimators of high-dimensional integrals over spaces.</li>
  <li>Evaluate the likelihood of data observed at test time (this can be used for rejection sampling or to score how good our model is).</li>
  <li>Find the conditional relationship between variables. For example, learning the distribution $p(x_2\vert x_1)$ allows us to build discriminative classification or regression models.</li>
  <li>Score our algorithm by using complexity measures like entropy, mutual information, and moments of the distribution.</li>
</ol>

<p><strong>Inference</strong>: $x\sim p_{X}$, $z=f(x)$.<br />
<strong>Generation</strong>: $z\sim p_{Z}$, $x=f^{-1}(z)$</p>

<p>How can we use a model $p(\mathbf{x}, \mathbf{z})$ to analyze some data $\mathbf{x}$? In other words, what hidden structure of $\mathbf{z}$ explains the data? We seek to infer this hidden structure using the model.</p>

<p>One method of inference leverages Bayes’ rule to define the posterior
\[p(\mathbf{z} \mid \mathbf{x}) = \frac{p(\mathbf{x}, \mathbf{z})}{\int_{z} p(\mathbf{x}, \mathbf{z}) \text{d}\mathbf{z}}.\]
​​ 
The posterior is the distribution of the latent variables $\mathbf{z}$, conditioned on some (observed) data $\mathbf{x}$. Drawing analogy to representation learning, it is a probabilistic description of the data’s hidden representation. Often this distribution is unknown and difficult to compute due to intractable denominator (which has to be computed over all comfiguration of states). Hence, we go for approximations.</p>

<p>$2$ methods exist: Sampling methods a.k.a. Monte-Carlo methods and Variational inference.</p>

<p><strong>Sampling methods</strong> involves drawing random samples from an unknown true distribution. Some methods are Importance sampling, Rejection sampling, Metropolis-hastings method and Gibbs sampling. The last two mthods falls under category of MCMC methods.</p>

<p><strong>Variational methods</strong> involves approximating the unknown true distribution $p(\mathbf{x})$ with a simpler known distribution $q(\mathbf{x};\theta)$ parametrized by $\theta$. Basically we minimize the divergence between these distributions over $\theta$. To approximate $p(\mathbf{x})$, <em>variational lower bound</em> is the key. We then maximize this VB to approximate $q$ to $p$. Its connections with deeplearning are beautifully established in paper - <a href="https://arxiv.org/abs/1312.6114">Auto-encoding Variational Bayes (Kingma &amp; Welling, 2014)</a>.</p>

<p>More on these methods could be read in the excellent book by David Mckay - `Information theory, Inference and Learning Algorithms’ and from blogs <a href="https://medium.com/neuralspace/inference-in-probabilistic-models-monte-carlo-and-deterministic-methods-eae8800ee095">1</a>, <a href="http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html">2</a>.</p>

<hr />

<h3 id="example-use-of-variational-inference-to-obtain-lower-bound-on-the-marginal-likelihood">Example: Use of Variational Inference to obtain lower-bound on the marginal likelihood</h3>
<p>Consider a general probabilistic model with observations $x$, latent variables $z$ over which we must integrate, and model parameters $θ$. We introduce an approximate posterior distribution for the latent variables $q_{\phi}(z\vert x)$ and follow the variational principle (Jordan et al., 1999) to obtain a bound on the marginal likelihood:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*} \log p_{θ}(x) & = \log \int p_{θ}(x\vert z)p(z)dz \\ 
                              & = \log \int \frac{q_{\phi}(z\vert x)}{q_{\phi}(z\vert x)} p_{θ}(x\vert z)p(z)dz \\
                              & = \log \left( \mathbb{E}_{q} \left[ \frac{p( z)}{q_{\phi}(z\vert x)} . q_{\phi}(z\vert x) p_{θ}(x\vert z) \right] \right) \\
                              & \geq \mathbb{E}_{q} \left[\log \left(\frac{p( z)}{q_{\phi}(z\vert x)} . q_{\phi}(z\vert x) p_{θ}(x\vert z)\right) \right]  \\
                              & \geq \mathbb{E}_{q} \left[\log \left(\frac{p( z)}{q_{\phi}(z\vert x)}\right) + \log\left(q_{\phi}(z\vert x) p_{θ}(x\vert z)\right) \right]  \\
                              & = -\mathbb{D}_{\mathrm{KL}} [q_{\phi}(z\vert x) \Vert p( z)] + \mathbb{E}_q [\log p(x\vert z)],
        \end{align*} %]]></script>

<p>where we have used Jensen’s inequality $ \left(f (\mathbb{E}[x]) ≤ \mathbb{E} [f(x)]\right) $. $p_θ (x\vert z)$ is a likelihood function and $p(z)$ is a prior over the latent variables. This bound is often referred to as the negative free energy $\mathcal{F}$ or as the evidence lower bound (ELBO). It consists of two terms: the first is the KL-divergence between the approximate posterior and the prior distribution (which acts as a regularizer), and the second is a reconstruction error. This bound provides a unified objective function for optimization of both the parameters $θ$ and $\phi$ of the model and variational approximation, respectively.</p>

<p>Current best practice in variational inference performs this optimization using mini-batches and stochastic gradient descent, which is what allows variational inference to be scaled to problems with very large data sets. There are two problems that must be addressed to successfully use the variational approach:</p>

<ol>
  <li>efficient computation of the derivatives of the expected log-likelihood <script type="math/tex">\nabla_{\phi} \mathbb{E}_{q_{\phi}(z)}\left[\log p_{\theta} (x\vert z)\right]</script>. This is tackled in the paper <a href="https://arxiv.org/abs/1312.6114">Auto-encoding Variational Bayes (Kingma &amp; Welling, 2014)</a></li>
  <li>choosing the richest, computationally-feasible approximate posterior distribution $q(·)$. This is tackled in the paper <a href="/post/checksum/">Variational Inference of Normalizing Flows (Rezende &amp; Shakir, 2015)</a>.</li>
</ol>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/post/latexsublime-workflow/">Latex+Sublime+MathJax+Jekyll workflow</a></div>
      <p class="meta"><a class="permalink" href="/post/latexsublime-workflow/">&#9679;</a> 11 May 2019</p>
      <div>
        <p>So basically to make heavy math websites faster, use the workflow as below:</p>

<ol>
  <li>Have the pre-requisites installed first i.e. latex using miktex and Sublime text 3.</li>
  <li>Make sure you have Perl also on your system.</li>
  <li>Add “latexindent.pl” package from miktex.</li>
  <li>Now set-up your sublime text as follows:
    <ul>
      <li>First things first: install ‘package control’</li>
      <li>Then add the following packages: LaTexTools, LateXYZ, Jekyll, BeautifyLatex</li>
      <li>Optionally add these packages just to make your experience better: materialtheme, side bar, advanced new file, file icon and GitGutter.</li>
    </ul>
  </li>
  <li>If needed, use Pandoc to convert tex file to html</li>
</ol>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">pandoc</span> <span class="o">-</span><span class="n">s</span> <span class="n">name_of_tex_file</span><span class="o">.</span><span class="n">tex</span> <span class="o">--</span><span class="n">mathjax</span> <span class="o">-</span><span class="n">o</span> <span class="n">name_of_ex_file</span><span class="o">.</span><span class="n">html</span> </code></pre></figure>

<p>That’s it. Your set-up is done!</p>

<p>To work faster with Sublime, learning keybindings (a.k.a keyboard shortcuts) is a must.</p>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/post/FactOnFuncAna/">LMI Formulation</a></div>
      <p class="meta"><a class="permalink" href="/post/FactOnFuncAna/">&#9679;</a> 01 May 2019</p>
      <div>
        <html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Arun Pandey" />
  <title>LMI formulation for Deep Learning</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="introduction">Introduction</h1>
<p>Representing the objective function in energy form: <span class="math display">\[\begin{aligned}
        \underset{w_1, w_2}{min} &amp; J = &amp; -x^\top  w_1 h_1 - \sigma(h_1)^\top  w_2 h_2.
    \end{aligned}\]</span> <span class="math inline">\( J \)</span> could be written in quadratic form as:</p>
<p><span class="math display">\[J=
    -\dfrac{1}{2}\begin{bmatrix}
        x^\top &amp; h_{1}^\top &amp; \sigma(h_1)^\top &amp; h_{2}^\top
    \end{bmatrix}
    \begin{bmatrix}
        \ast       &amp; w_{1} &amp; \ast       &amp; \ast \\
        w_{1}^\top &amp; \ast  &amp; \ast       &amp; \ast \\
        \ast       &amp; \ast  &amp; \ast       &amp; w_2  \\
        \ast       &amp; \ast  &amp; w_{2}^\top &amp; \ast \\
    \end{bmatrix}
    \begin{bmatrix}
        x \\ h_{1} \\ \sigma\left(h_1\right) \\ h_{2}
    \end{bmatrix}\]</span></p>
<p>Now, we characterizing the objective <span class="math inline">\( J \)</span> in terms of <span class="math inline">\( 1^{st}\)</span> layer using parameter <span class="math inline">\( \gamma \)</span>, the augmented objective is given by: <span class="math display">\[J \leq -\gamma x^\top w_1 h_1,\]</span></p>
<p>for some <span class="math inline">\( \gamma \gt 0 \)</span>. Using the sector <span class="math inline">\( \left[0,1\right] \)</span> non-linearity <span class="math display">\[\sigma\left(h_1 \right)^\top  \left( \sigma(h_1) - h_1 \right) \leq 0,~~~\forall h_1\]</span> and s-procedure trick, we obtain the following:</p>
<p><span class="math display">\[-(1-\gamma)x^\top w_1 h_1 - \sigma(h_1)^\top w_{2}h_{2}- \sigma\left(h_1 \right)^\top  \left( \sigma(h_1) - h_1 \right) \leq 0. ~~~\forall x, h_{1}, h_{2}\]</span> Hence, we solve the following optimization problem: <span class="math display">\[\begin{aligned}
        \underset{\gamma, w_{1}, w_{2}, h_{1_{i}}}{min} &amp; \quad -\gamma - \sum_{i}^{}x_{i}^T w_{1}h_{1_{i}} \\
        s.t.                                            &amp; -\dfrac{1}{2}\begin{bmatrix}
            x^\top &amp; h_{1}^\top &amp; \sigma(h_1)^\top &amp; h_{2}^\top
        \end{bmatrix}
        \begin{bmatrix}
            \ast                 &amp; (1-\gamma)w_{1} &amp; \ast       &amp; \ast \\
            (1-\gamma)w_{1}^\top &amp; \ast            &amp; -\mathbb{I}    &amp; \ast \\
            \ast                 &amp; -\mathbb{I}         &amp; 2\mathbb{I}    &amp; w_2  \\
            \ast                 &amp; \ast            &amp; w_{2}^\top &amp; \ast \\
        \end{bmatrix}
        \begin{bmatrix}
            x \\ h_{1} \\ \sigma\left(h_1\right) \\ h_{2}
        \end{bmatrix} \leq 0,                                                                   \\
                                                        &amp; \quad \gamma \gt 0.
    \end{aligned}\]</span> Since the first constraint is true <span class="math inline">\(\forall~ x,~ h_{1}, ~\sigma(h_{1}) ~\&amp; ~h_{2} \)</span>, we only need the matrix to be positive semi-definite. Hence, restating the above problem: <span class="math display">\[\begin{aligned}
        \underset{\gamma, w_{1}, w_{2}, h_{1_{i}}}{min} &amp; \quad -\gamma - \sum_{i}^{}x_{i}^\top w_{1}h_{1_{i}} \\
        s.t.                                            &amp; \qquad
        P \succeq 0                                                                                            \\
                                                        &amp; \qquad \gamma \gt 0
    \end{aligned}\]</span> where <span class="math inline">\( P:= \begin{bmatrix}
        \ast                 &amp; (1-\gamma)w_{1} &amp; \ast       &amp; \ast \\
        (1-\gamma)w_{1}^\top &amp; \ast            &amp; -\mathbb{I}    &amp; \ast \\
        \ast                 &amp; -\mathbb{I}         &amp; 2\mathbb{I}    &amp; w_2  \\
        \ast                 &amp; \ast            &amp; w_{2}^\top &amp; \ast \\
    \end{bmatrix} \)</span>.</p>
<h1 id="regularization">Regularization</h1>
<p>Regularizing the above problem to improve the structure of matrix. <span class="math display">\[\begin{aligned}
        \underset{w_1, w_2}{min} &amp; \tilde{J} = &amp; -x^\top  w_1 h_1 - \sigma(h_1)^\top  w_2 h_2 -\dfrac{\eta_{1}}{2}x^T x -\dfrac{\eta_{2}}{2}h_{1}^T h_{1} -\dfrac{\eta_{4}}{2}h_{2}^T h_{2}, ~~~~ \forall \eta_{1},~\eta_{2},~\eta_{4}\gt0
    \end{aligned}\]</span> and <span class="math inline">\( \eta_{3}\sigma\left(h_1 \right)^\top  \left( \sigma(h_1) - h_1 \right) \leq 0,~~~\forall h_1 , \eta_{3}\geq 0 \)</span>, we have the following optimization problem</p>
<p><span class="math display">\[\begin{aligned}
        \underset{\gamma, w_{1}, w_{2}, h_{1_{i}}}{min} &amp; \quad -\gamma - \sum_{i}^{}x_{i}^\top w_{1}h_{1_{i}} \\
        s.t.                                            &amp; \qquad
        \tilde{P} \succeq 0                                                                                    \\
                                                        &amp; \qquad \gamma \gt 0
    \end{aligned}\]</span></p>
<p>where <span class="math inline">\( \tilde{P}:= \begin{bmatrix}
        \eta_{1}             &amp; (1-\gamma)w_{1} &amp; \ast            &amp; \ast     \\
        (1-\gamma)w_{1}^\top &amp; \eta_{2}        &amp; -\eta_{3}\mathbb{I} &amp; \ast     \\
        \ast                 &amp; -\eta_{3}\mathbb{I} &amp; 2\eta_{3}\mathbb{I} &amp; w_2      \\
        \ast                 &amp; \ast            &amp; w_{2}^\top      &amp; \eta_{4} \\
    \end{bmatrix} \)</span>.</p>
<h2 id="from-bmi-to-lmi">From BMI to LMI</h2>
<p>Due to bilinearity in <span class="math inline">\( \tilde{P} \)</span>, the problem is in general NP-hard to solve. Hence a workaround is to impose a stronger condition on positive definiteness in the following way: <span class="math display">\[\tilde{P} = \tilde{P_{1}} + \tilde{P_{2}},\]</span> where <span class="math inline">\( \tilde{P_{1}} = \begin{bmatrix}
        \dfrac{\eta_{1}}{2} &amp; w_{1}               &amp; \ast            &amp; \ast     \\
        w_{1}^\top          &amp; \dfrac{\eta_{2}}{2} &amp; -\eta_{3}\mathbb{I} &amp; \ast     \\
        \ast                &amp; -\eta_{3}\mathbb{I}     &amp; 2\eta_{3}\mathbb{I} &amp; w_2      \\
        \ast                &amp; \ast                &amp; w_{2}^\top      &amp; \eta_{4} \\
    \end{bmatrix} \)</span> and <span class="math inline">\( \tilde{P_{2}} = \begin{bmatrix}
        \dfrac{\eta_{1}}{2} &amp; -\gamma w_{1}       &amp; \ast &amp; \ast \\
        -\gamma w_{1}^\top  &amp; \dfrac{\eta_{2}}{2} &amp; \ast &amp; \ast \\
        \ast                &amp; \ast                &amp; \ast &amp; \ast \\
        \ast                &amp; \ast                &amp; \ast &amp; \ast \\
    \end{bmatrix}. \)</span><br />
<br />
Left multiplying <span class="math inline">\( \tilde{P_{2}} \)</span> with <span class="math inline">\( \gamma^{-1} \)</span>, we get <span class="math inline">\( \tilde{P_{2}}^{&#39;} := \begin{bmatrix}
        \gamma^{-1}\dfrac{\eta_{1}}{2} &amp; - w_{1}                        &amp; \ast &amp; \ast \\
        - w_{1}^\top                   &amp; \gamma^{-1}\dfrac{\eta_{2}}{2} &amp; \ast &amp; \ast \\
        \ast                           &amp; \ast                           &amp; \ast &amp; \ast \\
        \ast                           &amp; \ast                           &amp; \ast &amp; \ast \\
    \end{bmatrix}.\)</span><br />
<br />
Hence, given <span class="math inline">\( \tilde{P_{1}} \succeq 0 \)</span> and <span class="math inline">\( \tilde{P_{2}} \succeq 0, \)</span> implies <span class="math inline">\( \tilde{P} \succeq 0\)</span>. Doing the change of variables <span class="math inline">\( L=w_{1}h_{1} \)</span>, to remove the bi-linearity from objective function and noting that <span class="math inline">\( {\left\lVertL\right\rVert}\leq{\left\lVertw_{1}\right\rVert} {\left\lVerth_{1}\right\rVert} \)</span> <span class="math inline">\( \leq {\left\lVertw_{1}\right\rVert} {\left\lVertx\right\rVert} \)</span>, since <span class="math inline">\( h_{1} \)</span> are just the latent variables of <span class="math inline">\( x \)</span>, we have the following reformulated problem</p>
<p><span class="math display">\[\label{eq: 2}
    \boxed{\begin{aligned}
            \underset{\gamma, w_{1}, w_{2}, L}{min} &amp; \quad -\gamma - Tr\left( x^\top L \right) \\
            s.t.                                    &amp; \qquad
            \tilde{P_{1}} \succeq 0                                                             \\
                                                    &amp; \qquad
            \tilde{P_{2}}^{&#39;} \succeq 0                                                         \\
                                                    &amp; \qquad \gamma \gt 0              \\
                                                    &amp; {\left\lVertL\right\rVert}\leq {\left\lVertw_{1}\right\rVert} {\left\lVertx\right\rVert}
        \end{aligned} }\]</span></p>
<p>Solving equation would yield <span class="math inline">\( \gamma,~  w_{1},~  w_{2}~ \&amp; ~ L \)</span>. Since <span class="math inline">\( w_{1} \in \mathbb{R}^{d\times s_{1}} \)</span> and <span class="math inline">\( h_{1} \in \mathbb{R}^{s_{1} \times N} \)</span>, we solve:</p>
<p><span class="math display">\[\begin{cases}
        Underdetermined ~system ~({\tiny no~ sol^{n}/infinitely~ many~ sol^{n}_{s}}),           &amp; \text{if } d\textless s_{1}    \\
        Overdetermined ~system~({\tiny mostly~ no ~sol^{n} ~except~ when~ lin.~ dep.~ exists}), &amp; \text{if } d\gt s_{1} \\
        Consistent ~system~(\tiny unique~ sol^{n} ~exists),                                     &amp; \text{if } d=s_{1}
    \end{cases}\]</span></p>
<p>Other than the consistent case, by choosing <span class="math inline">\( d\leq s_{1}  \)</span>, we prefer to solve the underdetermined system in the following way: <span class="math display">\[\begin{aligned}
        \underset{h_{1}}{min} &amp; \quad {\left\lVerth_{1}\right\rVert}^{2}_{2} \\
        s.t.                  &amp; \quad
        w_{1}h_{1}=L
    \end{aligned}\]</span> Solving the Lagrangian formulation and using KKT conditions we obtain</p>
<p><span class="math display">\[\label{}
    \boxed{h_{1}= w_{1}^{\top}\left( w_{1}w_{1}^{\top} \right)^{-1}L}\]</span></p>
<p>Now differentiating <span class="math inline">\( \tilde{J} \)</span> w.r.t <span class="math inline">\( h_{2} \)</span>, we obtain the solution for <span class="math inline">\( h_{2} \)</span> <span class="math display">\[\label{}
    \boxed{h_{2}=-\dfrac{1}{\eta_{4}}w_{2}^{\top}\sigma(h_{1})}\]</span></p>
</body>
</html>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/post/tes/">Manifolds learning and diffusion maps</a></div>
      <p class="meta"><a class="permalink" href="/post/tes/">&#9679;</a> 01 May 2019</p>
      <div>
        <h1 id="manifold-learning">Manifold learning</h1>

<p>Manifold learning is basically using the geometric properties of data to exploit machine learning. It is frequently used in dimensionality reduction (KPCA, LLE, diffusion maps etc), clustering and semi-supervised/supervised learning. It has strong connections with differential geometry and assumes the data lies near a manifold embedded in high-dimensional space.</p>

<hr />

<h1 id="diffusion-maps">Diffusion Maps</h1>
<p>Diffusion maps approximate some differential operators on data manifold $\mathcal{M}$. (proofs in the literature show this in the $\lim N\rightarrow \infty, \epsilon \rightarrow 0$). Here, we take the Taylor expansions of the functions to arrive at differential operators.</p>

<hr />

<h1 id="why-is-laplacian-special">Why is Laplacian special??</h1>

<ul>
  <li>The usage of this operator comes up in various daily-life problems: heat equation, wave equation, fuild-flow, quantum mechanics etc &amp; more recently in manifold learning (that’s what this blog summarizes!).</li>
  <li>It is invariant to <em>translation  &amp; rotation</em>. In general if $S$ is an operator which satisfies such properties, then $ S=\sum_{j=1}^{m}a_{j}\Delta^{j} $.</li>
</ul>

<h3 id="solving-deltapsilambdapsi-">Solving $\Delta\psi=\lambda\psi$ !</h3>
<p>We want to solve $\Delta\psi=\lambda\psi$. A standard method (inspired by Stone-Weierstrass Theorem) is to look for solutions
$ u(x; t) $ of the form $u(x; t) = \alpha(t)\phi(x)$. If you do this into heat equation, we get</p>

<script type="math/tex; mode=display">\dfrac{\Delta\phi(x)}{\phi(x)}= - \dfrac{\Delta\alpha(x)}{\alpha(x)}</script>

        </div>
             
	
    	<span class="label label-default"><a href="/tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  


<ul class="pager">
  
      <li class="previous"><a href="/">&larr; Newer</a></li>
  
    
      <li class="next"><a href="/page3">Older &rarr;</a></li>
  
</ul>
</section>





</body>
</html>
