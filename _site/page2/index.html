<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="icon" type="image/icon" href="favicon.ico">


<title>Home</title>


<script src="https://storage.googleapis.com/montco-stats/javascript/vis-4.17.0/dist/vis.js" type="text/javascript"></script>
<link href="https://storage.googleapis.com/montco-stats/javascript/vis-4.17.0/dist/vis-network.min.css" rel="stylesheet" type="text/css" />

<link rel="stylesheet" href="http://localhost:4000/css/syntax.css">
<link rel="stylesheet" href="http://localhost:4000/css/main.css">
</head>
<body>


<header>
<div id="nav_container">
<nav>

  <ul>
  <h1><a href="/about/">Research Notes</a></h1>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/about/">About</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/tutorials/">Tutorials</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/paper_summary/">Paper Summaries</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/distill/">Distills</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/tags/">Archives</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/almanac/personal_log/">Almanac</a>
      </li>
    
  </ul>

</nav>
</div>
</header>

<section id="main">
	  
      <article>
      <div class="heading"><a href="/about//post/latent-space-cartography-lie-et-al-2019/">Latent Space Cartography (Liu et. al, 2019)</a></div>
      <p class="meta"><a class="permalink" href="/about//post/latent-space-cartography-lie-et-al-2019/">&#9679;</a> 03 Sep 2019</p>
      <div>
        <p><a href="https://idl.cs.washington.edu/files/2019-LatentSpaceCartography-EuroVis.pdf">Paper Link</a></p>

<p>Employes latent-space visualization techniques (t-SNE, UMAP, and PCA) and contributes to methods eastbalishing semantic relationship between latent-space and the generated model. In particular, they contribute</p>
<ol>
  <li>
    <p>a review of domain literature and a characterization of common latent-space interpretation goals and tasks;</p>
  </li>
  <li>
    <p>a visual analysis system that supports these tasks, including novel projection strategies, visual and statistical methods to assess attribute vector uncertainty, and global attribute vector comparison methods; and</p>
  </li>
  <li>
    <p>three case studies demonstrating new insights into diverse data types across multiple domains.</p>
  </li>
</ol>

<p>In the literature, there exists 6 interpretation sub-tasks:</p>

<ol>
  <li>
    <p><strong>View Reconstruction Examples:</strong> A majority of articles on generative models (31/54) present a list of example generation results. These examples serve as qualitative evidence that the models produce compelling, plausible, yet novel outputs unseen in the training data. They also show reconstruction fidelity versus generation diversity, as there is usually a trade-off between the two.</p>
  </li>
  <li>
    <p><strong>View Interpolation Results:</strong> Linear interpolation in a continuous latent space is done by following a path between two points and displaying outputs at sampled locations on the path (usually in steps of equal distance). A number of articles (18/54) adopt this approach. Interpolation sequences naturally show the smoothness of the latent space, and they are important indicators of interpretability. For instance, authors may point out subtle changes in transitions that reflect deeper domain-specific principles</p>
  </li>
  <li>
    <p><strong>Examine Nearest Neighbors:</strong> This task is employed by articles on both word embeddings (10/24) and generative models (6/54), but the emphasis is different. Word embedding articles use
anecdotal nearest neighbors to qualitatively show what the algorithms manage to achieve. Articles on generative models use nearest neighbors from the training set as a comparison, demonstrating
that the algorithms generate novel results indistinguishable from, or better than, the input.</p>
  </li>
  <li>
    <p><strong>Perform Attribute Vector Arithmetic:</strong> An example of this task for word embeddings shows that the vector(king) - vector(man) + vector(woman) produces a vector very close to vector(queen).
This simple algebraic operation demonstrates that pairs of data points sharing a particular relationship have approximately constant vector offsets. We observe this task in both generative (3/54)
and word embedding (10/24) models.</p>
  </li>
  <li>
    <p><strong>Compare Similarities:</strong> This task is popular for benchmarks
of word embeddings (13/24), testing how well the cosine similarity
between word vectors matches ratings from human annotators.</p>
  </li>
  <li>
    <p><strong>Visualize Distribution:</strong> Researchers visualize latent spaces
using various means described in §3. This task is common to both
generative (20/54) and word embedding (7/24) settings.</p>
  </li>
</ol>


        </div>
             
	
    	<span class="label label-default"><a href="/about//tags#paper_summary-ref" title="View posts tagged with &quot;paper_summary&quot;">paper_summary</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/about//post/energy-based-learning/">Energy-based learning</a></div>
      <p class="meta"><a class="permalink" href="/about//post/energy-based-learning/">&#9679;</a> 03 Sep 2019</p>
      <div>
        <h2 id="ising-model">Ising model</h2>

<p>Ref. pg 400 of book Information theory, Inference, and Learning Algorithms.</p>

<p>Ising models are important first as models of magnetic systems that have
a phase transition. The theory of universality in statistical physics shows that
all systems with the same dimension (here, two), and the same symmetries,
have equivalent critical properties, i.e., the scaling laws shown by their phase
transitions are identical. So by studying Ising models we can find out not only
about magnetic phase transitions but also about phase transitions in many
other systems.</p>

<p>General form of Ising model is known as ‘spin glasses’ to physicists, and as ‘Hopfield networks’ or ‘Boltzmann machines’ to the neural network community. In some of these models, all spins are declared to be neighbours of each other, in which case physicists call the system an ‘infinite-range’ spin glass, and networkers call it a ‘fully connected’ network.</p>

<p><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf">A Tutorial on Energy-based Learning, Yann LeCunn, 2004</a></p>


        </div>
             
	
    	<span class="label label-default"><a href="/about//tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/about//post/latent-variable-models/">Latent variable models</a></div>
      <p class="meta"><a class="permalink" href="/about//post/latent-variable-models/">&#9679;</a> 30 Aug 2019</p>
      <div>
        <p>Many statistical models that are generative models (that is, models that specify a full probability density over all variables in the situation) make use of latent variables to describe a probability distribution over observables.</p>

<p>Some examples of latent variable models are mixture models, which model the observables as coming from a superposed mixture of simple probability distributions (the latent variables are the unknown class labels of the examples); hidden Markov models (Rabiner and Juang, 1986; Durbin et al., 1998); Probabilistic PCA; Gaussian-Process Latent variable model (GP-LVM, <a href="https://papers.nips.cc/paper/2540-gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data.pdf">Lawrence 2005</a>) and factor analysis. More recent ones that are popularized by ML community are RBMs, VAEs and GANs.</p>

<p>The decoding problem for error-correcting codes can also be viewed in terms of a latent variable model - figure below. In that case, the encoding matrix $G$ is normally known in advance. In latent variable modelling, the parameters equivalent to $G$ are usually not known, and must be inferred from the data along with the latent variables $s$. In the figure, the $K$ latent variables are the independent source bits $s_1, \dots , s_K$; these give rise to the observables via the generator matrix $G$.</p>

<p><img src="/photos/latent_var.PNG" alt="" style="width: 250px; height: auto;" /></p>

<p>Usually, the latent variables have a simple distribution, often a separable distribution. Thus when we fit a latent variable model, we are finding a description of the data in terms of ‘independent components’. The ‘<em>independent component analysis</em>’ algorithm corresponds to perhaps the simplest possible latent variable model with continuous latent variables.</p>

<hr />

<h3 id="independent-component-analysis">Independent Component Analysis</h3>

<p>Some references: <a href="/post/latent-variable-models/">tutorial</a> and  Ch. 34 in David Mckay - `Information theory, Inference and Learning Algorithms’.</p>

<hr />

<h3 id="footnotes">Footnotes:</h3>
<ol>
  <li>
    <p>MLE of ICA has the same form as Normalizing flows. Perhaps, NFs could be exploited to computed ICA ?!</p>
  </li>
  <li>
    <p>Is there a connection between ICA and KPCA?? The eigenvectors are independent, but what can we say about the non-gaussianity?!</p>
  </li>
</ol>


        </div>
             
	
    	<span class="label label-default"><a href="/about//tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  
      <article>
      <div class="heading"><a href="/about//post/jekyll-blog-with-oauth/">Jekyll blog with OAuth</a></div>
      <p class="meta"><a class="permalink" href="/about//post/jekyll-blog-with-oauth/">&#9679;</a> 28 Aug 2019</p>
      <div>
        <p>Refer these blogs <a href="https://ben.balter.com/jekyll-auth/">1</a>, <a href="https://blog.heroku.com/jekyll-on-heroku">2</a>.
Combine the ideas from both.</p>

        </div>
             
	
    	<span class="label label-default"><a href="/about//tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    


     </article>
     <hr>
  


<ul class="pager">
  
      <li class="previous"><a href="/about/">&larr; Newer</a></li>
  
    
      <li class="next"><a href="/about/page3">Older &rarr;</a></li>
  
</ul>
</section>





</body>
</html>
