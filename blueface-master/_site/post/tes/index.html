<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="icon" type="image/icon" href="favicon.ico">

<title>Manifolds learning and diffusion maps</title>

<link rel="stylesheet" href="http://localhost:4000/css/syntax.css">
<link rel="stylesheet" href="http://localhost:4000/css/main.css">
</head>
<body>


<header>
<div id="nav_container">
<nav>

  <ul>
  <h1><a href="">Research Notes</a></h1>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/about/">About</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/tutorials/">Tutorials</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/paper_summary/">Paper Summaries</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/archives/">Archives</a>
      </li>
    
      
      

      <li class="">
        <a class="" href="http://localhost:4000/tags/">Tags</a>
      </li>
    
  </ul>

</nav>
</div>
</header>

<section id="main">
	<article>
<div class="heading">Manifolds learning and diffusion maps</div>
<p class="meta"><a class="permalink" href="">&#9679;</a> 01 May 2019</p>
<h1 id="manifold-learning">Manifold learning</h1>

<p>Manifold learning is basically using the geometric properties of data to exploit machine learning. It is frequently used in dimensionality reduction (KPCA, LLE, diffusion maps etc), clustering and semi-supervised/supervised learning. It has strong connections with differential geometry and assumes the data lies near a manifold embedded in high-dimensional space.</p>

<hr />

<h1 id="diffusion-maps">Diffusion Maps</h1>
<p>Diffusion maps approximate some differential operators on data manifold $\mathcal{M}$. (proofs in the literature show this in the $\lim N\rightarrow \infty, \epsilon \rightarrow 0$). Here, we take the Taylor expansions of the functions to arrive at differential operators.</p>

<hr />

<h1 id="why-is-laplacian-special">Why is Laplacian special??</h1>

<ul>
  <li>The usage of this operator comes up in various daily-life problems: heat equation, wave equation, fuild-flow, quantum mechanics etc &amp; more recently in manifold learning (thatâ€™s what this blog summarizes!).</li>
  <li>It is invariant to <em>translation  &amp; rotation</em>. In general if $S$ is an operator which satisfies such properties, then $ S=\sum_{j=1}^{m}a_{j}\Delta^{j} $.</li>
</ul>

<h3 id="solving-deltapsilambdapsi-">Solving $\Delta\psi=\lambda\psi$ !</h3>
<p>We want to solve $\Delta\psi=\lambda\psi$. A standard method (inspired by Stone-Weierstrass Theorem) is to look for solutions
$ u(x; t) $ of the form $u(x; t) = \alpha(t)\phi(x)$. If you do this into heat equation, we get</p>

<script type="math/tex; mode=display">\dfrac{\Delta\phi(x)}{\phi(x)}= - \dfrac{\Delta\alpha(x)}{\alpha(x)}</script>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>



	
    	<span class="label label-default"><a href="/tags#distill-ref" title="View posts tagged with &quot;distill&quot;">distill</a></span>
    

</article>
</section>

</body>
</html>
